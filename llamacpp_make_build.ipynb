{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAMLiP85om8h",
        "outputId": "faf42b25-c094-4bc8-cb9e-58d723a43947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 20779, done.\u001b[K\n",
            "remote: Counting objects: 100% (7025/7025), done.\u001b[K\n",
            "remote: Compressing objects: 100% (369/369), done.\u001b[K\n",
            "remote: Total 20779 (delta 6848), reused 6717 (delta 6656), pack-reused 13754\u001b[K\n",
            "Receiving objects: 100% (20779/20779), 25.27 MiB | 16.96 MiB/s, done.\n",
            "Resolving deltas: 100% (14689/14689), done.\n",
            "/content/llama.cpp\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6voLCbYo3l-",
        "outputId": "9ba51412-4261-4f80-9d67-bf6de32d21ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c llama.cpp -o llama.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/common.cpp -o common.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/sampling.cpp -o sampling.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/build-info.cpp -o build-info.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/console.cpp -o console.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c unicode.cpp -o unicode.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/main/main.cpp -o examples/main/main.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/main/main.o -o main  \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/quantize/quantize.o -o quantize  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/quantize-stats/quantize-stats.o -o quantize-stats  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/perplexity/perplexity.o -o perplexity  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/imatrix/imatrix.o -o imatrix  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/embedding/embedding.o -o embedding  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o pocs/vdot/vdot.o -o vdot  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o pocs/vdot/q8dot.o -o q8dot  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/train.cpp -o train.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/train-text-from-scratch/train-text-from-scratch.o -o train-text-from-scratch  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o convert-llama2c-to-ggml  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/simple/simple.cpp -o examples/simple/simple.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/simple/simple.o -o simple  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/batched/batched.cpp -o examples/batched/batched.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/batched/batched.o -o batched  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  build-info.o ggml.o llama.o common.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/batched-bench/batched-bench.o -o batched-bench  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/save-load-state/save-load-state.o -o save-load-state  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/json-schema-to-grammar.cpp -o json-schema-to-grammar.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/server/server.cpp -o examples/server/server.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  json-schema-to-grammar.o ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o -Iexamples/server examples/server/server.o -o server   \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/gguf/gguf.o -o gguf  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/llama-bench/llama-bench.o -o llama-bench  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/llava/llava.cpp -o examples/llava/llava.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llava-cli  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/baby-llama/baby-llama.o -o baby-llama  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/beam-search/beam-search.cpp -o examples/beam-search/beam-search.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/beam-search/beam-search.o -o beam-search  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/speculative/speculative.o -o speculative  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/infill/infill.cpp -o examples/infill/infill.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/infill/infill.o -o infill  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/tokenize/tokenize.o -o tokenize  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  build-info.o ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/benchmark/benchmark-matmult.o -o benchmark-matmult  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/parallel/parallel.o -o parallel  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/finetune/finetune.o -o finetune  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/export-lora/export-lora.o -o export-lora  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/lookahead/lookahead.o -o lookahead  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/lookup/lookup.o -o lookup  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/passkey/passkey.o -o passkey  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/gritlm/gritlm.cpp -o examples/gritlm/gritlm.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o examples/gritlm/gritlm.o -o gritlm  \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"huggingface_hub[cli]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKYSDZSaqIWM",
        "outputId": "62a7493b-122e-4c5e-d47c-4f747745a78f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Collecting huggingface_hub[cli]\n",
            "  Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (24.0)\n",
            "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.43)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2024.2.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
            "Installing collected packages: pfzy, InquirerPy, huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed InquirerPy-0.3.4 huggingface_hub-0.21.4 pfzy-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download mistralai/Mistral-7B-v0.1 --local-dir ./models --exclude \"*.bin\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT-Ar5CRo3oz",
        "outputId": "5327fd72-9716-4b9a-899e-1aa6bce5dfd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "Fetching 12 files:   0% 0/12 [00:00<?, ?it/s]downloading https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/26bca36bde8333b5d7f72e9ed20ccda6a618af24/generation_config.json to /root/.cache/huggingface/hub/tmp6b2xqsmm\n",
            "downloading https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/26bca36bde8333b5d7f72e9ed20ccda6a618af24/.gitattributes to /root/.cache/huggingface/hub/tmpz0vup7uv\n",
            "downloading https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/26bca36bde8333b5d7f72e9ed20ccda6a618af24/model-00001-of-00002.safetensors to /root/.cache/huggingface/hub/tmp9_hlgvm8\n",
            "downloading https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/26bca36bde8333b5d7f72e9ed20ccda6a618af24/model-00002-of-00002.safetensors to /root/.cache/huggingface/hub/tmpwbxqg74c\n",
            "downloading https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/26bca36bde8333b5d7f72e9ed20ccda6a618af24/README.md to /root/.cache/huggingface/hub/tmp32vyms58\n",
            "downloading https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/26bca36bde8333b5d7f72e9ed20ccda6a618af24/config.json to /root/.cache/huggingface/hub/tmpz8q9jqbv\n",
            "downloading https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/26bca36bde8333b5d7f72e9ed20ccda6a618af24/model.safetensors.index.json to /root/.cache/huggingface/hub/tmpcivshofg\n",
            "\n",
            "generation_config.json: 100% 116/116 [00:00<00:00, 34.3kB/s]\n",
            "downloading https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/26bca36bde8333b5d7f72e9ed20ccda6a618af24/pytorch_model.bin.index.json to /root/.cache/huggingface/hub/tmptsrikgle\n",
            "\n",
            "config.json: 100% 571/571 [00:00<00:00, 2.03MB/s]\n",
            "\n",
            "README.md: 100% 1.39k/1.39k [00:00<00:00, 6.17MB/s]\n",
            "\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 5.95MB/s]\n",
            "Fetching 12 files:   8% 1/12 [00:00<00:07,  1.45it/s]\n",
            "model.safetensors.index.json:   0% 0.00/25.1k [00:00<?, ?B/s]\u001b[Adownloading https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/26bca36bde8333b5d7f72e9ed20ccda6a618af24/special_tokens_map.json to /root/.cache/huggingface/hub/tmp2tq5797u\n",
            "model.safetensors.index.json: 100% 25.1k/25.1k [00:00<00:00, 7.44MB/s]\n",
            "\n",
            "pytorch_model.bin.index.json: 100% 23.9k/23.9k [00:00<00:00, 24.2MB/s]\n",
            "downloading https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/26bca36bde8333b5d7f72e9ed20ccda6a618af24/tokenizer.json to /root/.cache/huggingface/hub/tmpd3xbemkh\n",
            "downloading https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/26bca36bde8333b5d7f72e9ed20ccda6a618af24/tokenizer.model to /root/.cache/huggingface/hub/tmpf0ho7tgu\n",
            "downloading https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/26bca36bde8333b5d7f72e9ed20ccda6a618af24/tokenizer_config.json to /root/.cache/huggingface/hub/tmp7cx362s8\n",
            "\n",
            "special_tokens_map.json: 100% 72.0/72.0 [00:00<00:00, 292kB/s]\n",
            "\n",
            "tokenizer_config.json: 100% 967/967 [00:00<00:00, 3.31MB/s]\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.94G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.model:   0% 0.00/493k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.json:   0% 0.00/1.80M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 4.10MB/s]\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 10.5M/4.54G [00:00<01:46, 42.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/9.94G [00:00<03:44, 44.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 5.73MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 5.48MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/9.94G [00:00<03:22, 49.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 31.5M/4.54G [00:00<01:20, 56.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 31.5M/9.94G [00:00<02:49, 58.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 41.9M/9.94G [00:00<02:33, 64.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 52.4M/4.54G [00:00<01:01, 73.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/9.94G [00:00<02:19, 71.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 73.4M/4.54G [00:01<00:55, 80.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/9.94G [00:00<02:25, 68.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 83.9M/4.54G [00:01<01:00, 74.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 73.4M/9.94G [00:01<02:32, 64.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 94.4M/4.54G [00:01<01:05, 68.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 83.9M/9.94G [00:01<02:40, 61.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 105M/4.54G [00:01<01:11, 62.1MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 94.4M/9.94G [00:01<02:52, 57.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 115M/4.54G [00:01<01:07, 65.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 105M/9.94G [00:01<02:51, 57.5MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 126M/4.54G [00:01<01:03, 69.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 115M/9.94G [00:01<02:35, 63.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 136M/4.54G [00:02<00:59, 73.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 126M/9.94G [00:02<02:25, 67.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 147M/4.54G [00:02<00:57, 76.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 136M/9.94G [00:02<02:11, 74.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 157M/4.54G [00:02<00:54, 81.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 147M/9.94G [00:02<02:00, 81.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 157M/9.94G [00:02<01:53, 86.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 178M/4.54G [00:02<00:48, 90.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 168M/9.94G [00:02<01:49, 89.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 189M/4.54G [00:02<00:47, 92.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 199M/4.54G [00:02<00:50, 86.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 178M/9.94G [00:02<01:59, 81.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 189M/9.94G [00:02<01:59, 81.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 210M/4.54G [00:02<00:52, 83.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 199M/9.94G [00:02<02:05, 77.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 220M/4.54G [00:02<00:54, 79.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 210M/9.94G [00:03<02:08, 75.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 231M/4.54G [00:03<00:55, 77.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 220M/9.94G [00:03<02:01, 79.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 241M/4.54G [00:03<00:57, 75.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 241M/9.94G [00:03<01:48, 89.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 262M/4.54G [00:03<00:49, 86.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 252M/9.94G [00:03<01:45, 91.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 273M/4.54G [00:03<00:48, 88.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 262M/9.94G [00:03<01:44, 92.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 283M/4.54G [00:03<00:46, 90.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 273M/9.94G [00:03<01:42, 94.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 294M/4.54G [00:03<00:45, 92.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 283M/9.94G [00:03<01:40, 96.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 315M/4.54G [00:03<00:42, 99.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 304M/9.94G [00:03<01:33, 104MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 325M/4.54G [00:04<00:43, 97.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 315M/9.94G [00:04<01:36, 99.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 336M/4.54G [00:04<00:47, 88.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 325M/9.94G [00:04<01:51, 86.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 346M/4.54G [00:04<00:48, 86.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 336M/9.94G [00:04<02:05, 76.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 357M/4.54G [00:04<00:52, 80.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 346M/9.94G [00:04<02:08, 74.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 367M/4.54G [00:04<00:52, 79.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 357M/9.94G [00:04<02:03, 77.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 388M/4.54G [00:04<00:47, 88.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 367M/9.94G [00:04<01:55, 83.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 398M/4.54G [00:04<00:46, 89.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 377M/9.94G [00:04<01:49, 87.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 388M/9.94G [00:05<01:47, 88.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 409M/4.54G [00:05<00:46, 88.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 398M/9.94G [00:05<01:44, 91.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 419M/4.54G [00:05<00:45, 90.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 409M/9.94G [00:05<01:43, 91.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 440M/4.54G [00:05<00:44, 91.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 419M/9.94G [00:05<01:47, 88.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 451M/4.54G [00:05<00:46, 88.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 430M/9.94G [00:05<01:52, 84.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 461M/4.54G [00:05<00:48, 85.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 440M/9.94G [00:05<01:54, 82.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 472M/4.54G [00:05<00:49, 82.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 451M/9.94G [00:05<01:57, 81.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 482M/4.54G [00:05<00:48, 83.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 461M/9.94G [00:05<01:56, 81.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 493M/4.54G [00:06<00:48, 82.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 472M/9.94G [00:06<01:57, 80.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 503M/4.54G [00:07<04:00, 16.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 482M/9.94G [00:07<09:42, 16.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 514M/4.54G [00:08<03:01, 22.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 493M/9.94G [00:07<07:15, 21.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 503M/9.94G [00:08<05:31, 28.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 524M/4.54G [00:08<02:20, 28.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 524M/9.94G [00:08<03:35, 43.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 545M/4.54G [00:08<01:31, 43.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 545M/9.94G [00:08<02:44, 57.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 566M/4.54G [00:08<01:09, 56.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 566M/9.94G [00:08<02:14, 69.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 587M/4.54G [00:08<00:57, 68.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 577M/9.94G [00:08<02:05, 74.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 598M/4.54G [00:08<00:53, 74.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 587M/9.94G [00:08<01:57, 79.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 608M/4.54G [00:08<00:50, 78.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 619M/4.54G [00:09<00:48, 81.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 598M/9.94G [00:08<01:58, 79.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 629M/4.54G [00:09<00:45, 86.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 608M/9.94G [00:09<01:54, 81.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 640M/4.54G [00:09<00:44, 87.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 619M/9.94G [00:09<02:20, 66.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 650M/4.54G [00:12<05:18, 12.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 629M/9.94G [00:12<15:58, 9.72MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 640M/9.94G [00:12<11:58, 12.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 650M/9.94G [00:13<08:56, 17.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 661M/4.54G [00:13<05:51, 11.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 671M/9.94G [00:13<05:34, 27.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 671M/4.54G [00:13<04:20, 14.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 692M/4.54G [00:13<02:35, 24.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 682M/9.94G [00:13<05:20, 28.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 713M/4.54G [00:13<01:46, 36.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 692M/9.94G [00:13<04:20, 35.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 734M/4.54G [00:13<01:19, 47.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 713M/9.94G [00:13<03:06, 49.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 755M/4.54G [00:14<01:04, 58.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 734M/9.94G [00:14<02:31, 60.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 765M/4.54G [00:14<01:01, 61.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 744M/9.94G [00:14<02:19, 66.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 776M/4.54G [00:14<00:56, 66.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 755M/9.94G [00:14<02:10, 70.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 786M/4.54G [00:14<00:52, 72.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 776M/9.94G [00:14<01:57, 77.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 807M/4.54G [00:14<00:47, 79.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 786M/9.94G [00:14<01:51, 82.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 828M/4.54G [00:14<00:41, 90.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 807M/9.94G [00:14<01:37, 93.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 818M/9.94G [00:14<01:36, 94.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 849M/4.54G [00:15<00:38, 96.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 828M/9.94G [00:15<01:35, 95.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 839M/9.94G [00:15<01:35, 94.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 870M/4.54G [00:15<00:38, 95.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 849M/9.94G [00:15<01:34, 96.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 881M/4.54G [00:15<00:38, 95.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 870M/9.94G [00:15<01:30, 100MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 902M/4.54G [00:17<03:03, 19.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 881M/9.94G [00:17<09:27, 16.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 912M/4.54G [00:18<02:33, 23.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 891M/9.94G [00:17<07:28, 20.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 923M/4.54G [00:18<02:08, 28.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 902M/9.94G [00:18<05:55, 25.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 933M/4.54G [00:18<01:46, 33.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 912M/9.94G [00:18<04:44, 31.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 944M/4.54G [00:18<01:29, 40.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 923M/9.94G [00:18<03:51, 38.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 954M/4.54G [00:18<01:14, 48.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 933M/9.94G [00:18<03:15, 46.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 965M/4.54G [00:18<01:06, 54.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 944M/9.94G [00:18<02:46, 54.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 975M/4.54G [00:18<01:00, 59.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  10% 954M/9.94G [00:18<02:29, 60.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 986M/4.54G [00:18<00:56, 63.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  10% 965M/9.94G [00:18<02:17, 65.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 996M/4.54G [00:19<00:52, 67.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  10% 975M/9.94G [00:18<02:09, 69.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 1.02G/4.54G [00:19<00:42, 82.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  10% 996M/9.94G [00:19<01:48, 82.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 1.04G/4.54G [00:19<00:36, 94.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  10% 1.02G/9.94G [00:19<01:32, 96.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 1.06G/4.54G [00:19<00:34, 102MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  10% 1.04G/9.94G [00:19<01:26, 103MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 1.08G/4.54G [00:19<00:32, 108MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 1.06G/9.94G [00:19<01:25, 104MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 1.10G/4.54G [00:19<00:32, 106MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 1.08G/9.94G [00:19<01:26, 102MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 1.12G/4.54G [00:20<00:31, 107MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 1.10G/9.94G [00:20<01:24, 104MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 1.14G/4.54G [00:20<00:31, 108MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 1.12G/9.94G [00:20<01:22, 107MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 1.16G/4.54G [00:20<00:30, 110MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 1.14G/9.94G [00:20<01:21, 108MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 1.18G/4.54G [00:20<00:30, 112MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  12% 1.16G/9.94G [00:20<01:19, 110MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 1.21G/4.54G [00:20<00:29, 113MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  12% 1.18G/9.94G [00:20<01:19, 110MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 1.23G/4.54G [00:21<00:31, 107MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  12% 1.21G/9.94G [00:21<01:19, 110MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 1.25G/4.54G [00:21<00:30, 107MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  12% 1.23G/9.94G [00:21<01:20, 109MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  13% 1.25G/9.94G [00:22<04:21, 33.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 1.27G/4.54G [00:22<01:40, 32.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  13% 1.26G/9.94G [00:22<03:50, 37.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 1.29G/4.54G [00:23<01:18, 41.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  13% 1.27G/9.94G [00:23<03:21, 43.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 1.31G/4.54G [00:23<01:03, 51.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  13% 1.29G/9.94G [00:23<02:34, 56.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 1.33G/4.54G [00:23<00:52, 61.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  13% 1.31G/9.94G [00:23<02:07, 67.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.35G/4.54G [00:23<00:44, 71.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  13% 1.33G/9.94G [00:23<01:48, 79.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.37G/4.54G [00:23<00:39, 79.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  14% 1.35G/9.94G [00:23<01:39, 86.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.39G/4.54G [00:24<00:37, 84.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  14% 1.37G/9.94G [00:24<01:34, 90.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.41G/4.54G [00:24<00:47, 65.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  14% 1.39G/9.94G [00:27<09:08, 15.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.42G/4.54G [00:27<04:08, 12.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.43G/4.54G [00:28<03:19, 15.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  14% 1.41G/9.94G [00:27<07:44, 18.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 1.45G/4.54G [00:28<02:11, 23.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  14% 1.43G/9.94G [00:28<05:30, 25.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 1.47G/4.54G [00:28<01:34, 32.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 1.45G/9.94G [00:28<04:12, 33.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 1.48G/4.54G [00:28<01:24, 36.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 1.46G/9.94G [00:28<03:47, 37.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 1.47G/9.94G [00:28<03:18, 42.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 1.50G/4.54G [00:28<01:02, 48.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 1.49G/9.94G [00:28<02:38, 53.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 1.52G/4.54G [00:28<00:50, 59.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 1.50G/9.94G [00:29<02:35, 54.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.53G/4.54G [00:29<00:50, 59.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 1.51G/9.94G [00:29<02:27, 57.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.54G/4.54G [00:29<00:48, 62.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.55G/4.54G [00:29<00:44, 67.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 1.52G/9.94G [00:29<02:18, 61.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.56G/4.54G [00:31<03:17, 15.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 1.53G/9.94G [00:32<14:04, 9.96MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 1.57G/4.54G [00:32<04:01, 12.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 1.54G/9.94G [00:32<10:38, 13.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 1.58G/4.54G [00:33<03:02, 16.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 1.56G/9.94G [00:33<06:28, 21.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 1.60G/4.54G [00:33<01:55, 25.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 1.57G/9.94G [00:33<05:40, 24.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.61G/4.54G [00:33<01:40, 29.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.63G/4.54G [00:33<01:21, 35.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 1.59G/9.94G [00:33<03:51, 36.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.65G/4.54G [00:33<00:57, 50.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 1.61G/9.94G [00:33<02:51, 48.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.67G/4.54G [00:33<00:46, 61.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 1.64G/9.94G [00:33<02:26, 56.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.68G/4.54G [00:34<00:43, 65.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  17% 1.65G/9.94G [00:34<02:13, 62.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  17% 1.66G/9.94G [00:34<02:02, 67.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.70G/4.54G [00:34<00:36, 78.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.72G/4.54G [00:34<00:32, 86.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  17% 1.68G/9.94G [00:34<01:45, 78.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  17% 1.69G/9.94G [00:34<01:40, 82.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.74G/4.54G [00:34<00:29, 93.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  17% 1.71G/9.94G [00:34<01:32, 89.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.76G/4.54G [00:34<00:27, 100MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  17% 1.73G/9.94G [00:34<01:23, 98.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.78G/4.54G [00:35<00:26, 104MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 1.75G/9.94G [00:35<01:21, 100MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.80G/4.54G [00:35<00:27, 100MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 1.77G/9.94G [00:35<01:20, 101MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.81G/4.54G [00:35<00:27, 98.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.84G/4.54G [00:35<00:26, 101MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 1.79G/9.94G [00:36<03:07, 43.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.85G/4.54G [00:37<02:21, 19.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 1.80G/9.94G [00:37<06:10, 22.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.87G/4.54G [00:38<01:38, 27.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 1.82G/9.94G [00:38<04:44, 28.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.88G/4.54G [00:38<01:29, 29.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 1.84G/9.94G [00:38<04:07, 32.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.89G/4.54G [00:38<01:15, 35.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 1.86G/9.94G [00:38<03:02, 44.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.91G/4.54G [00:38<00:56, 46.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.93G/4.54G [00:38<00:43, 60.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 1.87G/9.94G [00:38<03:14, 41.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.94G/4.54G [00:38<00:40, 63.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 1.88G/9.94G [00:38<02:51, 47.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.95G/4.54G [00:39<00:38, 66.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 1.90G/9.94G [00:39<02:10, 61.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.96G/4.54G [00:39<00:35, 72.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 1.91G/9.94G [00:39<01:59, 67.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.98G/4.54G [00:39<00:29, 86.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 1.92G/9.94G [00:39<01:49, 73.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 1.93G/9.94G [00:39<01:41, 78.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 2.00G/4.54G [00:39<00:26, 95.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 1.94G/9.94G [00:39<01:35, 83.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 2.02G/4.54G [00:39<00:24, 102MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 1.96G/9.94G [00:39<01:25, 93.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 2.04G/4.54G [00:39<00:22, 109MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 1.98G/9.94G [00:39<01:20, 98.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 2.07G/4.54G [00:40<00:22, 112MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 2.00G/9.94G [00:40<01:17, 103MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 2.09G/4.54G [00:40<00:21, 114MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 2.02G/9.94G [00:40<01:14, 106MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 2.11G/4.54G [00:40<00:22, 110MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 2.04G/9.94G [00:40<01:16, 103MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 2.13G/4.54G [00:40<00:22, 106MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 2.06G/9.94G [00:40<01:17, 102MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 2.07G/9.94G [00:40<01:18, 101MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 2.15G/4.54G [00:40<00:23, 104MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 2.08G/9.94G [00:40<01:18, 100MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 2.09G/9.94G [00:42<07:17, 17.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 2.10G/9.94G [00:42<05:43, 22.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 2.17G/4.54G [00:43<01:29, 26.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 2.18G/4.54G [00:43<01:21, 28.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 2.11G/9.94G [00:43<04:54, 26.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 2.19G/4.54G [00:43<01:09, 33.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 2.12G/9.94G [00:43<03:58, 32.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 2.21G/4.54G [00:43<00:50, 45.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 2.14G/9.94G [00:43<02:40, 48.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 2.23G/4.54G [00:43<00:39, 57.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 2.16G/9.94G [00:43<02:02, 63.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 2.24G/4.54G [00:43<00:36, 62.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 2.17G/9.94G [00:43<02:05, 62.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 2.25G/4.54G [00:43<00:34, 67.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 2.18G/9.94G [00:43<01:54, 68.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 2.26G/4.54G [00:44<00:31, 72.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 2.19G/9.94G [00:44<01:46, 73.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 2.28G/4.54G [00:44<00:30, 75.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 2.20G/9.94G [00:44<01:37, 79.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 2.29G/4.54G [00:44<00:29, 77.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 2.30G/4.54G [00:44<00:28, 78.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 2.22G/9.94G [00:44<01:27, 88.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 2.31G/4.54G [00:44<00:26, 83.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 2.23G/9.94G [00:44<01:24, 91.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 2.32G/4.54G [00:44<00:25, 87.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 2.24G/9.94G [00:44<01:22, 93.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 2.33G/4.54G [00:44<00:24, 90.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 2.25G/9.94G [00:44<01:27, 88.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 2.34G/4.54G [00:44<00:24, 90.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 2.26G/9.94G [00:44<01:25, 90.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 2.35G/4.54G [00:44<00:23, 93.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 2.28G/9.94G [00:44<01:29, 85.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 2.36G/4.54G [00:45<00:23, 94.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 2.29G/9.94G [00:45<01:25, 89.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 2.37G/4.54G [00:45<00:22, 96.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 2.38G/4.54G [00:45<00:23, 93.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 2.31G/9.94G [00:45<01:19, 96.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 2.32G/9.94G [00:45<01:18, 97.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 2.40G/4.54G [00:45<00:21, 98.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 2.34G/9.94G [00:45<01:12, 105MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 2.42G/4.54G [00:45<00:20, 105MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 2.36G/9.94G [00:45<01:08, 111MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 2.44G/4.54G [00:45<00:19, 110MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 2.38G/9.94G [00:45<01:10, 108MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 2.46G/4.54G [00:46<00:19, 105MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 2.47G/4.54G [00:46<00:21, 97.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 2.40G/9.94G [00:46<01:24, 89.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 2.49G/4.54G [00:46<00:23, 86.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 2.41G/9.94G [00:46<01:22, 91.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 2.50G/4.54G [00:46<00:22, 89.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 2.42G/9.94G [00:46<01:22, 91.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 2.43G/9.94G [00:47<05:00, 25.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 2.52G/4.54G [00:47<01:10, 28.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  25% 2.44G/9.94G [00:47<04:03, 30.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 2.53G/4.54G [00:48<00:58, 34.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  25% 2.46G/9.94G [00:48<02:51, 43.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 2.55G/4.54G [00:48<00:44, 44.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  25% 2.47G/9.94G [00:48<02:39, 46.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 2.56G/4.54G [00:48<00:41, 48.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  25% 2.49G/9.94G [00:48<02:17, 54.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 2.58G/4.54G [00:48<00:31, 62.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  25% 2.51G/9.94G [00:48<01:47, 69.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 2.60G/4.54G [00:48<00:26, 72.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  25% 2.53G/9.94G [00:48<01:40, 73.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.61G/4.54G [00:48<00:25, 75.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 2.54G/9.94G [00:49<01:37, 76.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.62G/4.54G [00:49<00:24, 77.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 2.55G/9.94G [00:49<01:40, 73.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.63G/4.54G [00:49<00:26, 71.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 2.56G/9.94G [00:49<01:33, 78.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.64G/4.54G [00:49<00:24, 76.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 2.57G/9.94G [00:49<01:28, 83.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.65G/4.54G [00:49<00:23, 81.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 2.58G/9.94G [00:50<03:07, 39.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.66G/4.54G [00:52<03:08, 9.96MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 2.59G/9.94G [00:52<11:36, 10.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.67G/4.54G [00:53<02:21, 13.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 2.60G/9.94G [00:53<08:43, 14.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.68G/4.54G [00:53<01:50, 16.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 2.61G/9.94G [00:53<06:52, 17.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 2.62G/9.94G [00:53<05:18, 23.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.69G/4.54G [00:53<01:25, 21.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.71G/4.54G [00:53<01:06, 27.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 2.63G/9.94G [00:53<04:16, 28.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.72G/4.54G [00:53<00:53, 33.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 2.64G/9.94G [00:53<03:29, 34.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.73G/4.54G [00:53<00:43, 41.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 2.65G/9.94G [00:53<02:55, 41.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.74G/4.54G [00:53<00:38, 47.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 2.66G/9.94G [00:53<02:29, 48.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.75G/4.54G [00:54<00:32, 54.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 2.67G/9.94G [00:54<02:09, 56.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.76G/4.54G [00:54<00:28, 61.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 2.68G/9.94G [00:54<01:52, 64.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.77G/4.54G [00:54<00:25, 69.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 2.69G/9.94G [00:54<01:40, 72.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.78G/4.54G [00:54<00:23, 76.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 2.71G/9.94G [00:54<01:37, 74.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.79G/4.54G [00:54<00:22, 77.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 2.72G/9.94G [00:54<01:37, 73.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 2.80G/4.54G [00:54<00:23, 73.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 2.73G/9.94G [00:54<01:39, 72.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 2.81G/4.54G [00:54<00:23, 72.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 2.74G/9.94G [00:54<01:41, 71.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 2.82G/4.54G [00:54<00:22, 77.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 2.75G/9.94G [00:54<01:32, 77.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 2.83G/4.54G [00:55<00:21, 79.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 2.76G/9.94G [00:55<01:25, 84.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.84G/4.54G [00:55<00:20, 84.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 2.77G/9.94G [00:55<01:21, 88.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.85G/4.54G [00:55<00:19, 87.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 2.78G/9.94G [00:55<01:19, 90.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.86G/4.54G [00:55<00:18, 90.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 2.79G/9.94G [00:55<01:17, 92.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.87G/4.54G [00:55<00:18, 91.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 2.80G/9.94G [00:55<01:16, 93.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.88G/4.54G [00:55<00:18, 90.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.89G/4.54G [00:55<00:17, 93.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 2.81G/9.94G [00:55<02:02, 58.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.90G/4.54G [00:55<00:18, 89.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.92G/4.54G [00:56<00:18, 88.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 2.82G/9.94G [00:55<01:55, 61.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.93G/4.54G [00:56<00:17, 91.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 2.83G/9.94G [00:56<01:41, 70.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 2.94G/4.54G [00:56<00:17, 93.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  29% 2.84G/9.94G [00:56<01:33, 75.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  29% 2.85G/9.94G [00:56<03:42, 31.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  29% 2.86G/9.94G [00:57<05:48, 20.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 2.96G/4.54G [00:58<01:10, 22.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 2.97G/4.54G [00:58<00:56, 27.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  29% 2.87G/9.94G [00:58<04:28, 26.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 2.98G/4.54G [00:58<00:45, 34.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  29% 2.89G/9.94G [00:58<02:53, 40.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 3.00G/4.54G [00:58<00:31, 48.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  29% 2.92G/9.94G [00:58<02:11, 53.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 3.02G/4.54G [00:58<00:24, 62.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 3.03G/4.54G [00:58<00:23, 65.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 2.94G/9.94G [00:58<01:52, 62.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 2.95G/9.94G [00:58<01:43, 67.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 3.05G/4.54G [00:58<00:18, 78.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 2.97G/9.94G [00:59<01:40, 69.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 3.07G/4.54G [00:59<00:18, 80.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 3.08G/4.54G [00:59<00:17, 81.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 2.98G/9.94G [00:59<01:39, 70.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 3.10G/4.54G [00:59<00:16, 89.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 3.00G/9.94G [00:59<01:24, 82.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 3.11G/4.54G [00:59<00:15, 92.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 3.02G/9.94G [00:59<01:15, 91.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 3.14G/4.54G [00:59<00:14, 98.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 3.04G/9.94G [00:59<01:10, 98.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 3.16G/4.54G [00:59<00:13, 102MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 3.06G/9.94G [00:59<01:13, 93.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 3.18G/4.54G [01:00<00:14, 97.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 3.07G/9.94G [01:00<01:13, 93.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 3.19G/4.54G [01:00<00:14, 96.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 3.08G/9.94G [01:00<01:12, 94.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 3.20G/4.54G [01:00<00:13, 97.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 3.09G/9.94G [01:00<01:12, 94.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 3.21G/4.54G [01:00<00:13, 98.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 3.10G/9.94G [01:00<01:11, 95.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 3.22G/4.54G [01:02<01:27, 15.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 3.11G/9.94G [01:02<07:51, 14.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 3.24G/4.54G [01:03<00:54, 23.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 3.14G/9.94G [01:03<04:56, 23.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 3.25G/4.54G [01:03<00:46, 28.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 3.15G/9.94G [01:03<04:15, 26.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 3.26G/4.54G [01:03<00:39, 32.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 3.16G/9.94G [01:03<03:32, 31.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 3.27G/4.54G [01:03<00:32, 39.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 3.17G/9.94G [01:03<02:53, 39.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 3.18G/9.94G [01:03<02:24, 46.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 3.29G/4.54G [01:03<00:22, 54.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 3.30G/4.54G [01:03<00:20, 61.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 3.20G/9.94G [01:03<01:52, 59.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 3.31G/4.54G [01:03<00:18, 65.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 3.21G/9.94G [01:03<01:42, 65.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 3.32G/4.54G [01:04<00:17, 70.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 3.22G/9.94G [01:04<01:35, 70.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 3.33G/4.54G [01:04<00:15, 76.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 3.23G/9.94G [01:04<01:28, 75.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 3.34G/4.54G [01:04<00:14, 82.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 3.25G/9.94G [01:04<01:15, 88.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 3.37G/4.54G [01:04<00:12, 91.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 3.27G/9.94G [01:04<01:08, 97.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 3.39G/4.54G [01:04<00:11, 99.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 3.29G/9.94G [01:04<01:04, 103MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 3.41G/4.54G [01:04<00:10, 105MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 3.31G/9.94G [01:04<01:01, 107MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 3.43G/4.54G [01:05<00:10, 108MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  34% 3.33G/9.94G [01:05<01:05, 101MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 3.45G/4.54G [01:05<00:11, 99.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  34% 3.34G/9.94G [01:05<01:08, 96.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 3.46G/4.54G [01:05<00:10, 98.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  34% 3.36G/9.94G [01:05<01:07, 97.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  34% 3.37G/9.94G [01:05<01:33, 70.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 3.48G/4.54G [01:06<00:26, 40.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  34% 3.38G/9.94G [01:07<06:58, 15.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 3.49G/4.54G [01:07<00:51, 20.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  34% 3.40G/9.94G [01:08<04:26, 24.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 3.51G/4.54G [01:08<00:35, 29.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 3.52G/4.54G [01:08<00:30, 33.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  34% 3.42G/9.94G [01:08<03:15, 33.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 3.53G/4.54G [01:08<00:26, 37.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  34% 3.43G/9.94G [01:08<02:53, 37.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 3.54G/4.54G [01:08<00:23, 43.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 3.45G/9.94G [01:08<02:09, 50.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 3.57G/4.54G [01:08<00:16, 58.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 3.59G/4.54G [01:08<00:13, 69.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 3.47G/9.94G [01:08<01:51, 57.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 3.48G/9.94G [01:09<01:43, 62.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 3.60G/4.54G [01:09<00:13, 71.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 3.49G/9.94G [01:09<01:35, 67.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 3.61G/4.54G [01:09<00:12, 74.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 3.50G/9.94G [01:09<01:29, 71.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 3.62G/4.54G [01:09<00:11, 78.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 3.63G/4.54G [01:09<00:11, 82.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 3.52G/9.94G [01:09<01:15, 85.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 3.65G/4.54G [01:09<00:09, 95.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  36% 3.54G/9.94G [01:09<01:07, 95.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 3.67G/4.54G [01:09<00:08, 99.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  36% 3.57G/9.94G [01:09<01:04, 98.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 3.68G/4.54G [01:09<00:08, 95.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  36% 3.58G/9.94G [01:09<01:06, 96.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 3.69G/4.54G [01:10<00:08, 95.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 3.70G/4.54G [01:10<00:08, 97.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  36% 3.60G/9.94G [01:10<01:03, 100MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  36% 3.61G/9.94G [01:10<01:03, 101MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 3.72G/4.54G [01:10<00:08, 99.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 3.73G/4.54G [01:12<00:38, 21.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  36% 3.63G/9.94G [01:12<05:41, 18.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 3.74G/4.54G [01:13<00:45, 17.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 3.75G/4.54G [01:13<00:36, 21.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  37% 3.65G/9.94G [01:13<04:15, 24.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 3.76G/4.54G [01:13<00:28, 27.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 3.77G/4.54G [01:13<00:22, 34.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  37% 3.66G/9.94G [01:13<03:39, 28.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 3.80G/4.54G [01:13<00:14, 49.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  37% 3.68G/9.94G [01:13<02:39, 39.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 3.82G/4.54G [01:13<00:11, 64.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  37% 3.70G/9.94G [01:13<02:03, 50.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 3.83G/4.54G [01:13<00:10, 68.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  37% 3.72G/9.94G [01:13<01:47, 57.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 3.84G/4.54G [01:14<00:10, 66.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 3.73G/9.94G [01:14<01:39, 62.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 3.74G/9.94G [01:14<01:32, 67.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 3.86G/4.54G [01:14<00:08, 76.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 3.76G/9.94G [01:14<01:17, 79.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 3.88G/4.54G [01:14<00:07, 84.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 3.77G/9.94G [01:14<01:18, 78.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 3.89G/4.54G [01:14<00:07, 84.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 3.90G/4.54G [01:14<00:07, 86.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 3.80G/9.94G [01:14<01:12, 84.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 3.91G/4.54G [01:14<00:07, 89.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 3.81G/9.94G [01:14<01:09, 87.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 3.92G/4.54G [01:14<00:06, 90.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 3.83G/9.94G [01:15<01:04, 95.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.94G/4.54G [01:15<00:06, 94.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  39% 3.84G/9.94G [01:15<01:04, 94.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.96G/4.54G [01:15<00:05, 103MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  39% 3.86G/9.94G [01:15<00:59, 103MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 3.98G/4.54G [01:15<00:05, 107MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  39% 3.88G/9.94G [01:15<00:56, 108MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 4.01G/4.54G [01:15<00:04, 109MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  39% 3.90G/9.94G [01:15<00:54, 110MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 4.03G/4.54G [01:15<00:04, 110MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  39% 3.92G/9.94G [01:15<00:55, 109MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 4.05G/4.54G [01:16<00:04, 100MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  40% 3.94G/9.94G [01:16<00:57, 104MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 4.06G/4.54G [01:16<00:04, 98.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 4.07G/4.54G [01:16<00:04, 99.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  40% 3.96G/9.94G [01:16<00:56, 106MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 4.08G/4.54G [01:16<00:04, 101MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  40% 3.98G/9.94G [01:16<00:56, 105MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 4.10G/4.54G [01:16<00:04, 105MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 4.11G/4.54G [01:16<00:04, 104MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  40% 4.01G/9.94G [01:16<00:55, 107MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 4.13G/4.54G [01:16<00:03, 110MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  40% 4.03G/9.94G [01:16<00:53, 111MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 4.15G/4.54G [01:17<00:03, 114MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 4.05G/9.94G [01:17<00:52, 112MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 4.17G/4.54G [01:17<00:03, 106MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 4.07G/9.94G [01:17<00:55, 107MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 4.09G/9.94G [01:17<00:56, 104MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 4.19G/4.54G [01:17<00:03, 99.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 4.20G/4.54G [01:17<00:03, 100MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 4.10G/9.94G [01:17<00:59, 98.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 4.22G/4.54G [01:17<00:03, 95.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 4.11G/9.94G [01:17<00:59, 97.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 4.23G/4.54G [01:17<00:03, 81.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 4.12G/9.94G [01:17<01:10, 82.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 4.24G/4.54G [01:18<00:03, 82.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 4.13G/9.94G [01:18<01:09, 83.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 4.25G/4.54G [01:18<00:03, 85.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 4.14G/9.94G [01:18<01:06, 87.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 4.26G/4.54G [01:18<00:03, 79.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 4.15G/9.94G [01:18<01:17, 75.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 4.27G/4.54G [01:18<00:03, 81.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 4.16G/9.94G [01:18<01:12, 80.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 4.29G/4.54G [01:18<00:02, 93.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 4.30G/4.54G [01:18<00:02, 95.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 4.17G/9.94G [01:18<01:38, 58.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 4.31G/4.54G [01:18<00:02, 97.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 4.18G/9.94G [01:18<01:29, 64.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 4.32G/4.54G [01:19<00:02, 91.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 4.19G/9.94G [01:18<01:27, 65.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 4.33G/4.54G [01:19<00:02, 92.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 4.20G/9.94G [01:19<01:37, 59.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 4.34G/4.54G [01:19<00:02, 73.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 4.35G/4.54G [01:19<00:02, 79.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 4.22G/9.94G [01:19<01:29, 64.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 4.36G/4.54G [01:19<00:02, 84.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 4.23G/9.94G [01:19<01:19, 71.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 4.25G/9.94G [01:19<01:05, 86.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 4.37G/4.54G [01:19<00:02, 57.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 4.27G/9.94G [01:19<00:58, 96.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 4.38G/4.54G [01:19<00:02, 65.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 4.28G/9.94G [01:19<00:59, 95.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 4.39G/4.54G [01:20<00:02, 67.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 4.29G/9.94G [01:20<01:03, 88.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 4.40G/4.54G [01:20<00:01, 71.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 4.30G/9.94G [01:20<01:06, 84.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 4.41G/4.54G [01:20<00:01, 67.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 4.31G/9.94G [01:20<01:09, 81.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 4.42G/4.54G [01:20<00:01, 72.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 4.33G/9.94G [01:20<01:04, 87.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 4.44G/4.54G [01:20<00:01, 73.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 4.45G/4.54G [01:22<00:07, 13.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 4.34G/9.94G [01:22<05:37, 16.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 4.36G/9.94G [01:23<03:40, 25.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 4.47G/4.54G [01:23<00:03, 22.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 4.37G/9.94G [01:23<03:16, 28.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 4.48G/4.54G [01:23<00:02, 25.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 4.39G/9.94G [01:23<02:17, 40.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 4.50G/4.54G [01:23<00:01, 37.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 4.41G/9.94G [01:23<01:44, 52.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 4.52G/4.54G [01:23<00:00, 50.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 4.53G/4.54G [01:23<00:00, 54.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  45% 4.44G/9.94G [01:23<01:35, 57.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 4.54G/4.54G [01:24<00:00, 54.0MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00002.safetensors:  45% 4.45G/9.94G [01:24<01:29, 61.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  45% 4.47G/9.94G [01:24<01:12, 75.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  45% 4.49G/9.94G [01:24<01:01, 88.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  45% 4.51G/9.94G [01:24<00:55, 97.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 4.53G/9.94G [01:24<00:51, 105MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 4.55G/9.94G [01:24<00:50, 108MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 4.57G/9.94G [01:25<00:48, 110MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 4.59G/9.94G [01:25<00:47, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 4.61G/9.94G [01:25<00:47, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  47% 4.63G/9.94G [01:25<00:46, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  47% 4.66G/9.94G [01:25<00:44, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  47% 4.68G/9.94G [01:25<00:43, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  47% 4.70G/9.94G [01:26<00:42, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  47% 4.72G/9.94G [01:26<00:42, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  48% 4.74G/9.94G [01:26<00:41, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  48% 4.76G/9.94G [01:26<00:42, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  48% 4.78G/9.94G [01:26<00:43, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  48% 4.80G/9.94G [01:26<00:43, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  49% 4.82G/9.94G [01:27<00:42, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  49% 4.84G/9.94G [01:27<00:42, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  49% 4.87G/9.94G [01:27<01:13, 69.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  49% 4.89G/9.94G [01:28<01:03, 79.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  49% 4.91G/9.94G [01:28<00:55, 90.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 4.93G/9.94G [01:28<00:50, 99.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 4.95G/9.94G [01:28<00:46, 108MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 4.97G/9.94G [01:28<00:45, 109MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 4.99G/9.94G [01:28<00:44, 113MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 5.01G/9.94G [01:29<00:43, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  51% 5.03G/9.94G [01:29<00:42, 116MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  51% 5.05G/9.94G [01:29<00:42, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  51% 5.08G/9.94G [01:29<00:41, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  51% 5.10G/9.94G [01:29<00:40, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  51% 5.12G/9.94G [01:29<00:40, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  52% 5.14G/9.94G [01:30<00:39, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  52% 5.16G/9.94G [01:30<00:38, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  52% 5.18G/9.94G [01:30<00:38, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  52% 5.20G/9.94G [01:30<00:39, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  53% 5.22G/9.94G [01:30<00:39, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  53% 5.24G/9.94G [01:31<00:40, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  53% 5.26G/9.94G [01:31<00:41, 113MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  53% 5.28G/9.94G [01:31<00:43, 108MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  53% 5.31G/9.94G [01:32<02:03, 37.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 5.33G/9.94G [01:33<01:37, 47.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 5.35G/9.94G [01:33<01:20, 56.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 5.37G/9.94G [01:33<01:06, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 5.39G/9.94G [01:33<00:57, 79.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 5.41G/9.94G [01:33<00:51, 88.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  55% 5.43G/9.94G [01:33<00:45, 98.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  55% 5.45G/9.94G [01:34<00:41, 107MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  55% 5.47G/9.94G [01:34<00:39, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  55% 5.49G/9.94G [01:34<00:37, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  55% 5.52G/9.94G [01:34<00:37, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  56% 5.54G/9.94G [01:34<00:37, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  56% 5.56G/9.94G [01:34<00:37, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  56% 5.58G/9.94G [01:35<00:37, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  56% 5.60G/9.94G [01:35<00:36, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 5.62G/9.94G [01:35<00:36, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 5.64G/9.94G [01:35<00:35, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 5.66G/9.94G [01:35<00:34, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 5.68G/9.94G [01:35<00:33, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 5.70G/9.94G [01:36<00:32, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  58% 5.73G/9.94G [01:36<00:33, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  58% 5.75G/9.94G [01:36<00:34, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  58% 5.77G/9.94G [01:36<00:34, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  58% 5.79G/9.94G [01:36<00:47, 86.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  58% 5.81G/9.94G [01:37<00:43, 94.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  59% 5.83G/9.94G [01:37<00:41, 100MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  59% 5.85G/9.94G [01:37<00:38, 105MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  59% 5.87G/9.94G [01:37<00:37, 109MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  59% 5.89G/9.94G [01:37<00:35, 116MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  59% 5.91G/9.94G [01:38<00:33, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  60% 5.93G/9.94G [01:38<00:32, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  60% 5.96G/9.94G [01:38<00:32, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  60% 5.98G/9.94G [01:38<00:31, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  60% 6.00G/9.94G [01:38<00:33, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 6.02G/9.94G [01:38<00:32, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 6.04G/9.94G [01:39<00:32, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 6.06G/9.94G [01:39<00:31, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 6.08G/9.94G [01:39<00:30, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 6.10G/9.94G [01:39<00:30, 128MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  62% 6.12G/9.94G [01:39<00:30, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  62% 6.14G/9.94G [01:39<00:30, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  62% 6.17G/9.94G [01:40<00:31, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  62% 6.19G/9.94G [01:40<00:32, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  62% 6.21G/9.94G [01:40<00:31, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  63% 6.23G/9.94G [01:40<00:30, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  63% 6.25G/9.94G [01:40<00:29, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  63% 6.27G/9.94G [01:40<00:28, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  63% 6.29G/9.94G [01:41<00:28, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  63% 6.31G/9.94G [01:41<00:28, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  64% 6.33G/9.94G [01:41<00:28, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  64% 6.35G/9.94G [01:41<00:29, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  64% 6.38G/9.94G [01:41<00:28, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  64% 6.40G/9.94G [01:41<00:28, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  65% 6.42G/9.94G [01:42<00:27, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  65% 6.44G/9.94G [01:42<00:27, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  65% 6.46G/9.94G [01:42<00:26, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  65% 6.48G/9.94G [01:42<00:26, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  65% 6.50G/9.94G [01:42<00:27, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 6.52G/9.94G [01:42<00:28, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 6.54G/9.94G [01:43<00:28, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 6.56G/9.94G [01:43<00:28, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 6.59G/9.94G [01:43<00:28, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 6.61G/9.94G [01:43<00:27, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  67% 6.63G/9.94G [01:43<00:27, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  67% 6.65G/9.94G [01:43<00:29, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  67% 6.67G/9.94G [01:44<00:30, 106MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  67% 6.69G/9.94G [01:44<00:30, 107MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  67% 6.71G/9.94G [01:44<00:30, 105MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  68% 6.73G/9.94G [01:44<00:30, 105MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  68% 6.75G/9.94G [01:44<00:29, 108MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  68% 6.77G/9.94G [01:45<00:28, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  68% 6.79G/9.94G [01:45<00:27, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  69% 6.82G/9.94G [01:45<00:26, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  69% 6.84G/9.94G [01:45<00:25, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  69% 6.86G/9.94G [01:45<00:25, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  69% 6.88G/9.94G [01:46<00:25, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  69% 6.90G/9.94G [01:46<00:25, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  70% 6.92G/9.94G [01:46<00:25, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  70% 6.94G/9.94G [01:46<00:25, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  70% 6.96G/9.94G [01:47<01:15, 39.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  70% 6.98G/9.94G [01:48<00:59, 49.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  70% 7.00G/9.94G [01:48<00:48, 61.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  71% 7.03G/9.94G [01:48<00:40, 72.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  71% 7.05G/9.94G [01:48<00:34, 84.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  71% 7.07G/9.94G [01:48<00:30, 93.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  71% 7.09G/9.94G [01:48<00:28, 99.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  72% 7.11G/9.94G [01:49<00:27, 104MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  72% 7.13G/9.94G [01:49<00:25, 109MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  72% 7.15G/9.94G [01:49<00:25, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  72% 7.17G/9.94G [01:49<00:24, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  72% 7.19G/9.94G [01:49<00:23, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  73% 7.21G/9.94G [01:49<00:22, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  73% 7.24G/9.94G [01:50<00:21, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  73% 7.26G/9.94G [01:50<00:21, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  73% 7.28G/9.94G [01:50<00:20, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  73% 7.30G/9.94G [01:50<00:20, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  74% 7.32G/9.94G [01:50<00:20, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  74% 7.34G/9.94G [01:50<00:20, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  74% 7.36G/9.94G [01:51<00:20, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  74% 7.38G/9.94G [01:51<00:20, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  74% 7.40G/9.94G [01:51<00:19, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  75% 7.42G/9.94G [01:51<00:19, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  75% 7.44G/9.94G [01:51<00:19, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  75% 7.47G/9.94G [01:51<00:19, 128MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  75% 7.49G/9.94G [01:52<00:19, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  76% 7.51G/9.94G [01:52<00:20, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  76% 7.53G/9.94G [01:52<00:20, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  76% 7.55G/9.94G [01:52<00:27, 86.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  76% 7.57G/9.94G [01:52<00:24, 96.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  76% 7.59G/9.94G [01:53<00:22, 105MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  77% 7.61G/9.94G [01:53<00:20, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  77% 7.63G/9.94G [01:53<00:19, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  77% 7.65G/9.94G [01:53<00:18, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  77% 7.68G/9.94G [01:53<00:19, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  77% 7.70G/9.94G [01:53<00:19, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  78% 7.72G/9.94G [01:54<00:18, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  78% 7.74G/9.94G [01:54<00:17, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  78% 7.76G/9.94G [01:54<00:17, 128MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  78% 7.78G/9.94G [01:54<00:17, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  78% 7.80G/9.94G [01:54<00:17, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79% 7.82G/9.94G [01:54<00:17, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79% 7.84G/9.94G [01:55<00:17, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79% 7.86G/9.94G [01:55<00:17, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79% 7.89G/9.94G [01:55<00:17, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  80% 7.91G/9.94G [01:55<00:17, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  80% 7.93G/9.94G [01:55<00:16, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  80% 7.95G/9.94G [01:56<00:16, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  80% 7.97G/9.94G [01:56<00:16, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  80% 7.99G/9.94G [01:56<00:17, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  81% 8.01G/9.94G [01:56<00:17, 110MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  81% 8.03G/9.94G [01:56<00:17, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  81% 8.05G/9.94G [01:56<00:16, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  81% 8.07G/9.94G [01:57<00:16, 116MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  81% 8.10G/9.94G [01:57<00:15, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  82% 8.12G/9.94G [01:57<00:15, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  82% 8.14G/9.94G [01:57<00:15, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  82% 8.16G/9.94G [01:57<00:16, 110MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  82% 8.18G/9.94G [01:58<00:15, 113MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  82% 8.20G/9.94G [01:58<00:14, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 8.22G/9.94G [01:58<00:14, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 8.24G/9.94G [01:58<00:13, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 8.26G/9.94G [01:58<00:13, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 8.28G/9.94G [01:58<00:13, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  84% 8.30G/9.94G [01:59<00:13, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  84% 8.33G/9.94G [01:59<00:13, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  84% 8.35G/9.94G [01:59<00:13, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  84% 8.37G/9.94G [01:59<00:13, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  84% 8.39G/9.94G [01:59<00:12, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  85% 8.41G/9.94G [01:59<00:12, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  85% 8.43G/9.94G [02:00<00:11, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  85% 8.45G/9.94G [02:00<00:11, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  85% 8.47G/9.94G [02:00<00:11, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  85% 8.49G/9.94G [02:00<00:11, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  86% 8.51G/9.94G [02:00<00:11, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  86% 8.54G/9.94G [02:00<00:11, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  86% 8.56G/9.94G [02:01<00:11, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  86% 8.58G/9.94G [02:01<00:11, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  86% 8.60G/9.94G [02:01<00:10, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  87% 8.62G/9.94G [02:01<00:10, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  87% 8.64G/9.94G [02:01<00:10, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  87% 8.66G/9.94G [02:01<00:09, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  87% 8.68G/9.94G [02:02<00:09, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  88% 8.70G/9.94G [02:02<00:09, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  88% 8.72G/9.94G [02:02<00:09, 128MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  88% 8.75G/9.94G [02:02<00:09, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  88% 8.77G/9.94G [02:02<00:09, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  88% 8.79G/9.94G [02:02<00:09, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  89% 8.81G/9.94G [02:03<00:09, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  89% 8.83G/9.94G [02:03<00:08, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  89% 8.85G/9.94G [02:03<00:08, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  89% 8.87G/9.94G [02:03<00:08, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  89% 8.89G/9.94G [02:03<00:08, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  90% 8.91G/9.94G [02:03<00:08, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  90% 8.93G/9.94G [02:04<00:08, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  90% 8.95G/9.94G [02:04<00:08, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  90% 8.98G/9.94G [02:04<00:07, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  90% 9.00G/9.94G [02:04<00:07, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  91% 9.02G/9.94G [02:04<00:07, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  91% 9.04G/9.94G [02:04<00:07, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  91% 9.06G/9.94G [02:05<00:07, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  91% 9.08G/9.94G [02:05<00:07, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 9.10G/9.94G [02:05<00:07, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 9.12G/9.94G [02:05<00:06, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 9.14G/9.94G [02:05<00:06, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 9.16G/9.94G [02:06<00:06, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 9.19G/9.94G [02:06<00:05, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  93% 9.21G/9.94G [02:06<00:05, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  93% 9.23G/9.94G [02:06<00:05, 128MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  93% 9.25G/9.94G [02:06<00:05, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  93% 9.27G/9.94G [02:06<00:05, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  93% 9.29G/9.94G [02:07<00:05, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  94% 9.31G/9.94G [02:07<00:05, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  94% 9.33G/9.94G [02:07<00:05, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  94% 9.35G/9.94G [02:07<00:04, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  94% 9.37G/9.94G [02:08<00:07, 77.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  94% 9.40G/9.94G [02:08<00:06, 86.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  95% 9.42G/9.94G [02:08<00:05, 94.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  95% 9.44G/9.94G [02:08<00:05, 100MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  95% 9.46G/9.94G [02:08<00:04, 105MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  95% 9.48G/9.94G [02:08<00:04, 106MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  96% 9.50G/9.94G [02:09<00:04, 103MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  96% 9.52G/9.94G [02:09<00:04, 103MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  96% 9.54G/9.94G [02:09<00:03, 103MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  96% 9.56G/9.94G [02:09<00:03, 106MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  96% 9.58G/9.94G [02:09<00:03, 110MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 9.60G/9.94G [02:10<00:02, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 9.63G/9.94G [02:10<00:02, 116MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 9.65G/9.94G [02:10<00:02, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 9.67G/9.94G [02:10<00:02, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 9.69G/9.94G [02:10<00:02, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  98% 9.71G/9.94G [02:11<00:01, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  98% 9.73G/9.94G [02:11<00:01, 117MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  98% 9.75G/9.94G [02:11<00:01, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  98% 9.77G/9.94G [02:11<00:01, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  98% 9.79G/9.94G [02:12<00:03, 37.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  99% 9.81G/9.94G [02:13<00:02, 47.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  99% 9.84G/9.94G [02:13<00:01, 58.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  99% 9.86G/9.94G [02:13<00:01, 69.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  99% 9.88G/9.94G [02:13<00:00, 81.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors: 100% 9.90G/9.94G [02:13<00:00, 91.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors: 100% 9.92G/9.94G [02:13<00:00, 99.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors: 100% 9.94G/9.94G [02:14<00:00, 74.0MB/s]\n",
            "Fetching 12 files: 100% 12/12 [02:15<00:00, 11.28s/it]\n",
            "/content/llama.cpp/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ih72Iyqo3rp",
        "outputId": "4b74c1b0-06e8-4bfe-ac68-3df3e6f3eed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy~=1.24.4 (from -r ./requirements/requirements-convert.txt (line 1))\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece~=0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert.txt (line 2)) (0.1.99)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert.txt (line 3)) (4.38.2)\n",
            "Collecting gguf>=0.1.0 (from -r ./requirements/requirements-convert.txt (line 4))\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r ./requirements/requirements-convert.txt (line 5))\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch~=2.1.1 (from -r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops~=0.7.0 (from -r ./requirements/requirements-convert-hf-to-gguf.txt (line 3))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.21.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.1.0 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n",
            "Installing collected packages: triton, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gguf, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed einops-0.7.0 gguf-0.6.0 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 protobuf-4.25.3 torch-2.1.2 triton-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 convert.py models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9cUVZxEo3uX",
        "outputId": "670b983f-94b3-4070-941c-d86e711248cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model file models/model-00001-of-00002.safetensors\n",
            "Loading model file models/model-00001-of-00002.safetensors\n",
            "Loading model file models/model-00002-of-00002.safetensors\n",
            "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models'))\n",
            "Found vocab files: {'spm': PosixPath('models/tokenizer.model'), 'bpe': None, 'hfft': PosixPath('models/tokenizer.json')}\n",
            "Loading vocab file PosixPath('models/tokenizer.model'), type 'spm'\n",
            "Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>\n",
            "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
            "Permuting layer 0\n",
            "Permuting layer 1\n",
            "Permuting layer 2\n",
            "Permuting layer 3\n",
            "Permuting layer 4\n",
            "Permuting layer 5\n",
            "Permuting layer 6\n",
            "Permuting layer 7\n",
            "Permuting layer 8\n",
            "Permuting layer 9\n",
            "Permuting layer 10\n",
            "Permuting layer 11\n",
            "Permuting layer 12\n",
            "Permuting layer 13\n",
            "Permuting layer 14\n",
            "Permuting layer 15\n",
            "Permuting layer 16\n",
            "Permuting layer 17\n",
            "Permuting layer 18\n",
            "Permuting layer 19\n",
            "Permuting layer 20\n",
            "Permuting layer 21\n",
            "Permuting layer 22\n",
            "Permuting layer 23\n",
            "Permuting layer 24\n",
            "Permuting layer 25\n",
            "Permuting layer 26\n",
            "Permuting layer 27\n",
            "Permuting layer 28\n",
            "Permuting layer 29\n",
            "Permuting layer 30\n",
            "Permuting layer 31\n",
            "model.embed_tokens.weight                        -> token_embd.weight                        | BF16   | [32000, 4096]\n",
            "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "lm_head.weight                                   -> output.weight                            | BF16   | [32000, 4096]\n",
            "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.norm.weight                                -> output_norm.weight                       | BF16   | [4096]\n",
            "Writing models/ggml-model-f32.gguf, format 0\n",
            "Ignoring added_tokens.json since model matches vocab size without it.\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "gguf: Setting special token type bos to 1\n",
            "gguf: Setting special token type eos to 2\n",
            "gguf: Setting special token type unk to 0\n",
            "gguf: Setting add_bos_token to True\n",
            "gguf: Setting add_eos_token to False\n",
            "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F32  | T+   3\n",
            "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+  14\n",
            "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  14\n",
            "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+  15\n",
            "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  16\n",
            "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+  17\n",
            "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  17\n",
            "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F32  | T+  17\n",
            "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  17\n",
            "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  18\n",
            "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  18\n",
            "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  19\n",
            "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+  20\n",
            "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  21\n",
            "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  22\n",
            "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  22\n",
            "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F32  | T+  22\n",
            "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  22\n",
            "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  22\n",
            "[ 20/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  22\n",
            "[ 21/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  24\n",
            "[ 22/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  24\n",
            "[ 23/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  25\n",
            "[ 24/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  26\n",
            "[ 25/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  26\n",
            "[ 26/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F32  | T+  26\n",
            "[ 27/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  26\n",
            "[ 28/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  27\n",
            "[ 29/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  27\n",
            "[ 30/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  27\n",
            "[ 31/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  30\n",
            "[ 32/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  34\n",
            "[ 33/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  40\n",
            "[ 34/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  40\n",
            "[ 35/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F32  | T+  40\n",
            "[ 36/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  40\n",
            "[ 37/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  40\n",
            "[ 38/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  40\n",
            "[ 39/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  40\n",
            "[ 40/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  41\n",
            "[ 41/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  43\n",
            "[ 42/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  44\n",
            "[ 43/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  44\n",
            "[ 44/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F32  | T+  44\n",
            "[ 45/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  45\n",
            "[ 46/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  45\n",
            "[ 47/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  45\n",
            "[ 48/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  45\n",
            "[ 49/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  46\n",
            "[ 50/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  48\n",
            "[ 51/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  50\n",
            "[ 52/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  50\n",
            "[ 53/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F32  | T+  50\n",
            "[ 54/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  50\n",
            "[ 55/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  54\n",
            "[ 56/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  54\n",
            "[ 57/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  54\n",
            "[ 58/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  59\n",
            "[ 59/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  60\n",
            "[ 60/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  61\n",
            "[ 61/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  61\n",
            "[ 62/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F32  | T+  61\n",
            "[ 63/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  61\n",
            "[ 64/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  62\n",
            "[ 65/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  62\n",
            "[ 66/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  62\n",
            "[ 67/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  64\n",
            "[ 68/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  64\n",
            "[ 69/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  65\n",
            "[ 70/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  65\n",
            "[ 71/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F32  | T+  65\n",
            "[ 72/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  65\n",
            "[ 73/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  66\n",
            "[ 74/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  66\n",
            "[ 75/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  67\n",
            "[ 76/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  69\n",
            "[ 77/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  70\n",
            "[ 78/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  71\n",
            "[ 79/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  71\n",
            "[ 80/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F32  | T+  71\n",
            "[ 81/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  71\n",
            "[ 82/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  72\n",
            "[ 83/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  72\n",
            "[ 84/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  72\n",
            "[ 85/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  74\n",
            "[ 86/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  74\n",
            "[ 87/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  75\n",
            "[ 88/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  75\n",
            "[ 89/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F32  | T+  75\n",
            "[ 90/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  76\n",
            "[ 91/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  76\n",
            "[ 92/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  76\n",
            "[ 93/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  76\n",
            "[ 94/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  78\n",
            "[ 95/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  79\n",
            "[ 96/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  79\n",
            "[ 97/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  79\n",
            "[ 98/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F32  | T+  79\n",
            "[ 99/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  80\n",
            "[100/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  80\n",
            "[101/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  80\n",
            "[102/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  83\n",
            "[103/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  85\n",
            "[104/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  90\n",
            "[105/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  95\n",
            "[106/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  95\n",
            "[107/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F32  | T+  95\n",
            "[108/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  96\n",
            "[109/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  97\n",
            "[110/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  97\n",
            "[111/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  97\n",
            "[112/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 101\n",
            "[113/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 104\n",
            "[114/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+ 105\n",
            "[115/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 105\n",
            "[116/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 106\n",
            "[117/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 106\n",
            "[118/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 106\n",
            "[119/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+ 106\n",
            "[120/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 106\n",
            "[121/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 109\n",
            "[122/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 111\n",
            "[123/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+ 115\n",
            "[124/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 115\n",
            "[125/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 115\n",
            "[126/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 115\n",
            "[127/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 116\n",
            "[128/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+ 116\n",
            "[129/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 116\n",
            "[130/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 117\n",
            "[131/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 119\n",
            "[132/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+ 120\n",
            "[133/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 120\n",
            "[134/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 120\n",
            "[135/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 120\n",
            "[136/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 120\n",
            "[137/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 120\n",
            "[138/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 120\n",
            "[139/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 121\n",
            "[140/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 121\n",
            "[141/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+ 121\n",
            "[142/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+ 124\n",
            "[143/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 125\n",
            "[144/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 126\n",
            "[145/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+ 127\n",
            "[146/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 127\n",
            "[147/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 127\n",
            "[148/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 127\n",
            "[149/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 127\n",
            "[150/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+ 127\n",
            "[151/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+ 128\n",
            "[152/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 129\n",
            "[153/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 135\n",
            "[154/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+ 135\n",
            "[155/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 135\n",
            "[156/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 135\n",
            "[157/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 136\n",
            "[158/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 136\n",
            "[159/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+ 136\n",
            "[160/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+ 136\n",
            "[161/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 138\n",
            "[162/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 140\n",
            "[163/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+ 141\n",
            "[164/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 141\n",
            "[165/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 141\n",
            "[166/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 141\n",
            "[167/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 141\n",
            "[168/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+ 141\n",
            "[169/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+ 141\n",
            "[170/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 143\n",
            "[171/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 144\n",
            "[172/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+ 149\n",
            "[173/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 149\n",
            "[174/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 149\n",
            "[175/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 150\n",
            "[176/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 150\n",
            "[177/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+ 150\n",
            "[178/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+ 150\n",
            "[179/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 151\n",
            "[180/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 152\n",
            "[181/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+ 153\n",
            "[182/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 153\n",
            "[183/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 153\n",
            "[184/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 153\n",
            "[185/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 153\n",
            "[186/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+ 153\n",
            "[187/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+ 154\n",
            "[188/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 156\n",
            "[189/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 156\n",
            "[190/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+ 157\n",
            "[191/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 157\n",
            "[192/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 157\n",
            "[193/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 157\n",
            "[194/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 158\n",
            "[195/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+ 158\n",
            "[196/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+ 160\n",
            "[197/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 161\n",
            "[198/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 161\n",
            "[199/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+ 162\n",
            "[200/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 162\n",
            "[201/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 162\n",
            "[202/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 163\n",
            "[203/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 163\n",
            "[204/291] Writing tensor output.weight                          | size  32000 x   4096  | type F32  | T+ 166\n",
            "[205/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+ 172\n",
            "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 172\n",
            "[207/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 173\n",
            "[208/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 173\n",
            "[209/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+ 174\n",
            "[210/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+ 174\n",
            "[211/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 174\n",
            "[212/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 176\n",
            "[213/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 177\n",
            "[214/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 178\n",
            "[215/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 178\n",
            "[216/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 178\n",
            "[217/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 178\n",
            "[218/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 179\n",
            "[219/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 179\n",
            "[220/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 179\n",
            "[221/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 180\n",
            "[222/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 181\n",
            "[223/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 182\n",
            "[224/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 182\n",
            "[225/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 182\n",
            "[226/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 183\n",
            "[227/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 183\n",
            "[228/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 183\n",
            "[229/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 185\n",
            "[230/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 186\n",
            "[231/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 192\n",
            "[232/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 197\n",
            "[233/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 197\n",
            "[234/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 197\n",
            "[235/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 197\n",
            "[236/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 197\n",
            "[237/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 197\n",
            "[238/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 197\n",
            "[239/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 198\n",
            "[240/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 200\n",
            "[241/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 202\n",
            "[242/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 202\n",
            "[243/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 202\n",
            "[244/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 203\n",
            "[245/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 203\n",
            "[246/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 203\n",
            "[247/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 203\n",
            "[248/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 207\n",
            "[249/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 208\n",
            "[250/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 213\n",
            "[251/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 213\n",
            "[252/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 213\n",
            "[253/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 213\n",
            "[254/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 213\n",
            "[255/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 214\n",
            "[256/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 214\n",
            "[257/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 214\n",
            "[258/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 217\n",
            "[259/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 217\n",
            "[260/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 217\n",
            "[261/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 217\n",
            "[262/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 218\n",
            "[263/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 218\n",
            "[264/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 218\n",
            "[265/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 218\n",
            "[266/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 220\n",
            "[267/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 222\n",
            "[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 223\n",
            "[269/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 223\n",
            "[270/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 223\n",
            "[271/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 223\n",
            "[272/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 224\n",
            "[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 224\n",
            "[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 224\n",
            "[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 225\n",
            "[276/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 227\n",
            "[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 227\n",
            "[278/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 227\n",
            "[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 227\n",
            "[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 228\n",
            "[281/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 228\n",
            "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 228\n",
            "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 228\n",
            "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 233\n",
            "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 236\n",
            "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 238\n",
            "[287/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 238\n",
            "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 238\n",
            "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 242\n",
            "[290/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 243\n",
            "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 243\n",
            "Wrote models/ggml-model-f32.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd llama.cpp\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRx9nyuctl53",
        "outputId": "31486c21-6e50-4e0f-a322-2fd32ed6d34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " baby-llama\t\t\t ggml.h\t\t\t       lookahead\n",
            " batched\t\t\t ggml-impl.h\t\t       lookup\n",
            " batched-bench\t\t\t ggml-kompute.cpp\t       main\n",
            " beam-search\t\t\t ggml-kompute.h\t\t       Makefile\n",
            " benchmark-matmult\t\t ggml-metal.h\t\t       media\n",
            " build-info.o\t\t\t ggml-metal.m\t\t       models\n",
            " build.zig\t\t\t ggml-metal.metal\t       mypy.ini\n",
            " ci\t\t\t\t ggml-mpi.c\t\t       Package.swift\n",
            " cmake\t\t\t\t ggml-mpi.h\t\t       parallel\n",
            " CMakeLists.txt\t\t\t ggml.o\t\t\t       passkey\n",
            " codecov.yml\t\t\t ggml-opencl.cpp\t       perplexity\n",
            " common\t\t\t\t ggml-opencl.h\t\t       pocs\n",
            " common.o\t\t\t ggml-quants.c\t\t       prompts\n",
            " console.o\t\t\t ggml-quants.h\t\t       q8dot\n",
            " convert-hf-to-gguf.py\t\t ggml-quants.o\t\t       quantize\n",
            " convert-llama2c-to-ggml\t ggml-sycl.cpp\t\t       quantize-stats\n",
            " convert-llama-ggml-to-gguf.py\t ggml-sycl.h\t\t       README.md\n",
            " convert-lora-to-ggml.py\t ggml_vk_generate_shaders.py   README-sycl.md\n",
            " convert-persimmon-to-gguf.py\t ggml-vulkan.cpp\t       requirements\n",
            " convert.py\t\t\t ggml-vulkan.h\t\t       requirements.txt\n",
            " docs\t\t\t\t ggml-vulkan-shaders.hpp       sampling.o\n",
            " embedding\t\t\t gguf\t\t\t       save-load-state\n",
            " examples\t\t\t gguf-py\t\t       scripts\n",
            " export-lora\t\t\t grammar-parser.o\t       server\n",
            " finetune\t\t\t grammars\t\t       simple\n",
            " flake.lock\t\t\t gritlm\t\t\t       speculative\n",
            " flake.nix\t\t\t imatrix\t\t       spm-headers\n",
            " ggml-alloc.c\t\t\t infill\t\t\t       tests\n",
            " ggml-alloc.h\t\t\t json-schema-to-grammar.o      tokenize\n",
            " ggml-alloc.o\t\t\t kompute\t\t       train.o\n",
            " ggml-backend.c\t\t\t kompute-shaders\t       train-text-from-scratch\n",
            " ggml-backend.h\t\t\t libllava.a\t\t       unicode.cpp\n",
            " ggml-backend-impl.h\t\t LICENSE\t\t       unicode.h\n",
            " ggml-backend.o\t\t\t llama-bench\t\t       unicode.o\n",
            " ggml.c\t\t\t\t llama.cpp\t\t      'Untitled Folder'\n",
            " ggml-common.h\t\t\t llama.h\t\t       vdot\n",
            " ggml-cuda.cu\t\t\t llama.o\n",
            " ggml-cuda.h\t\t\t llava-cli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ./quantize ./models/ggml-model-f32.gguf ./models/ggml-model-Q4_K_M.gguf Q4_K_M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvErOYWRo3w2",
        "outputId": "d028c6f0-70e1-41dd-9fd0-a087ec79621b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 2491 (fa046eaf)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing './models/ggml-model-f32.gguf' to './models/ggml-model-Q4_K_M.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/ggml-model-f32.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 0\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:  291 tensors\n",
            "llama_model_quantize_internal: meta size = 734944 bytes\n",
            "[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f32, converting to q4_K .. size =   500.00 MiB ->    70.31 MiB\n",
            "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[  20/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[  22/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  23/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  24/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  25/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  26/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  27/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  28/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[  29/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[  31/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  32/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  33/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  34/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  35/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  36/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  37/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[  38/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  40/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  41/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  42/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  43/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  44/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  45/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  46/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  47/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  49/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  50/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  51/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  52/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  53/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  54/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  55/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  56/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[  58/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  59/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  60/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  61/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  62/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  63/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  64/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[  65/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  67/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  68/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  69/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  70/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  71/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  72/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  73/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  74/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  76/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  77/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  78/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  79/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  80/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  81/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  82/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  83/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[  85/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  86/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  87/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  88/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  89/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  90/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  91/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[  92/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  94/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  95/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  96/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  97/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  98/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  99/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 100/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 101/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 103/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 104/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 105/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 106/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 107/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 108/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 109/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 110/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 112/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 113/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 114/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 115/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 116/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 117/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 118/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 119/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 121/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 122/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 123/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 124/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 125/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 126/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 127/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 128/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 130/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 131/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 132/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 133/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 134/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 135/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 136/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 137/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 138/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 139/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 140/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 141/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 142/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 143/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 144/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 145/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 146/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 147/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 148/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 149/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 150/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 151/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 152/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 153/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 154/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 155/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 156/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 157/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 158/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 159/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 160/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 161/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 162/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 163/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 164/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 165/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 166/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 167/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 168/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 169/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 170/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 171/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 172/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 173/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 174/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 175/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 176/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 177/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 178/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 179/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 180/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 181/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 182/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 183/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 184/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 185/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 186/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 187/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 188/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 189/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 190/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 191/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 192/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 193/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 194/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 195/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 196/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 197/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 198/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 199/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 200/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 201/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 202/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 203/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 204/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f32, converting to q6_K .. size =   500.00 MiB ->   102.54 MiB\n",
            "[ 205/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 206/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 207/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 208/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 211/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 212/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 213/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 214/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 215/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 216/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 217/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 218/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 219/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 220/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 221/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 222/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 223/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 224/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 225/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 226/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 227/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 228/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 229/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 230/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 231/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 232/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 233/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 234/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 235/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 236/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 237/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 238/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 239/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 240/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 241/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 242/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 243/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 244/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 245/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 246/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 247/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 248/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 249/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 250/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 251/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 252/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 253/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 254/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 255/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 256/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 257/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 258/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 259/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 260/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 261/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 262/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 263/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 264/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 265/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 266/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 267/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 268/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 269/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 270/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 271/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 272/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 273/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 274/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 275/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 276/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 277/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 278/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 279/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 280/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 281/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 282/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 283/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 284/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 285/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 286/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 287/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 288/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 289/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 290/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "llama_model_quantize_internal: model size  = 27625.02 MB\n",
            "llama_model_quantize_internal: quant size  =  4165.37 MB\n",
            "\n",
            "main: quantize time = 1048071.02 ms\n",
            "main:    total time = 1048071.02 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start inference on a gguf model\n",
        "!./main -m ./models/ggml-model-Q4_K_M.gguf -n 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQXi6yrpo3zW",
        "outputId": "a4785401-8448-4847-85c7-ff85c0a075e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log start\n",
            "main: build = 2491 (fa046eaf)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1711089756\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ./models/ggml-model-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = .\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            "...............................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    73.00 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1060\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 512, n_batch = 2048, n_predict = 128, n_keep = 1\n",
            "\n",
            "\n",
            " # 8 Things We Learned About The Future Of Mobile\n",
            "\n",
            "This article originally appeared on Forbes.\n",
            "\n",
            "In a world where smartphones are ubiquitous, it’s easy to forget how new this technology is. Just seven years ago, Apple introduced the iPhone.\n",
            "\n",
            "At the first annual Mobile World Congress in Barcelona this week, industry leaders gathered to share their predictions on the future of mobile technology. Here are eight things they learned about the future of mobile.\n",
            "\n",
            "1. Mobile is the first screen in the home, not the second.\n",
            "\n",
            "In 2013, more than 7\n",
            "llama_print_timings:        load time =   33306.26 ms\n",
            "llama_print_timings:      sample time =       7.59 ms /   128 runs   (    0.06 ms per token, 16862.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =   83703.18 ms /   128 runs   (  653.93 ms per token,     1.53 tokens per second)\n",
            "llama_print_timings:       total time =   83766.45 ms /   129 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfGKHN-To31-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ov7xXHG1o34q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c2TWiYzTo369"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WZUG61V6o39z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}