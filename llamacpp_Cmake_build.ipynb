{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICfyUrCS6Z1t",
        "outputId": "9f4f3d29-fa9e-4593-a0be-cc5ebfbf68c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 20804, done.\u001b[K\n",
            "remote: Counting objects: 100% (6479/6479), done.\u001b[K\n",
            "remote: Compressing objects: 100% (357/357), done.\u001b[K\n",
            "remote: Total 20804 (delta 6298), reused 6190 (delta 6122), pack-reused 14325\u001b[K\n",
            "Receiving objects: 100% (20804/20804), 23.83 MiB | 18.14 MiB/s, done.\n",
            "Resolving deltas: 100% (14696/14696), done.\n",
            "/content/llama.cpp\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!cmake --build . --config Release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6xgartv68k_",
        "outputId": "df401be0-72b7-4a2b-9a3e-090fc16c9e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp/build\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE  \n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Configuring done (1.0s)\n",
            "-- Generating done (0.1s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  1%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-backend.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-quants.c.o\u001b[0m\n",
            "[  2%] Built target ggml\n",
            "[  3%] \u001b[32m\u001b[1mLinking C static library libggml_static.a\u001b[0m\n",
            "[  3%] Built target ggml_static\n",
            "[  4%] \u001b[32mBuilding CXX object CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32m\u001b[1mLinking CXX static library libllama.a\u001b[0m\n",
            "[  5%] Built target llama\n",
            "[  5%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
            "[  6%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[  6%] Built target build_info\n",
            "[  7%] \u001b[32mBuilding CXX object common/CMakeFiles/json-schema-to-grammar.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[  7%] Built target json-schema-to-grammar\n",
            "[  8%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/train.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 11%] Built target common\n",
            "[ 12%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 13%] Built target test-quantize-fns\n",
            "[ 14%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 15%] Built target test-quantize-perf\n",
            "[ 16%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 17%] Built target test-sampling\n",
            "[ 18%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 19%] Built target test-chat-template\n",
            "[ 19%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/get-model.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0-llama\u001b[0m\n",
            "[ 21%] Built target test-tokenizer-0-llama\n",
            "[ 22%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/get-model.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0-falcon\u001b[0m\n",
            "[ 23%] Built target test-tokenizer-0-falcon\n",
            "[ 24%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/get-model.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-llama\u001b[0m\n",
            "[ 25%] Built target test-tokenizer-1-llama\n",
            "[ 25%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/test-tokenizer-1-llama.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-baichuan.dir/get-model.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-baichuan\u001b[0m\n",
            "[ 27%] Built target test-tokenizer-1-baichuan\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-falcon.dir/get-model.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-falcon\u001b[0m\n",
            "[ 28%] Built target test-tokenizer-1-falcon\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-aquila.dir/get-model.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-aquila\u001b[0m\n",
            "[ 30%] Built target test-tokenizer-1-aquila\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-mpt.dir/get-model.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-mpt\u001b[0m\n",
            "[ 32%] Built target test-tokenizer-1-mpt\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-stablelm-3b-4e1t.dir/get-model.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-stablelm-3b-4e1t\u001b[0m\n",
            "[ 34%] Built target test-tokenizer-1-stablelm-3b-4e1t\n",
            "[ 35%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-gpt-neox.dir/get-model.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-gpt-neox\u001b[0m\n",
            "[ 36%] Built target test-tokenizer-1-gpt-neox\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-refact.dir/get-model.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-refact\u001b[0m\n",
            "[ 38%] Built target test-tokenizer-1-refact\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-starcoder.dir/get-model.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-starcoder\u001b[0m\n",
            "[ 40%] Built target test-tokenizer-1-starcoder\n",
            "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-gpt2.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-gpt2\u001b[0m\n",
            "[ 42%] Built target test-tokenizer-1-gpt2\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 44%] Built target test-grammar-parser\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 45%] Built target test-llama-grammar\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grad0\u001b[0m\n",
            "[ 47%] Built target test-grad0\n",
            "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 49%] Built target test-backend-ops\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 51%] Built target test-rope\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 53%] Built target test-model-load-cancel\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 55%] Built target test-autorelease\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 57%] Built target test-json-schema-to-grammar\n",
            "[ 57%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-c\u001b[0m\n",
            "[ 58%] Built target test-c\n",
            "[ 58%] \u001b[32mBuilding CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/baby-llama\u001b[0m\n",
            "[ 59%] Built target baby-llama\n",
            "[ 59%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/batched\u001b[0m\n",
            "[ 60%] Built target batched\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/batched-bench\u001b[0m\n",
            "[ 61%] Built target batched-bench\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/beam-search\u001b[0m\n",
            "[ 63%] Built target beam-search\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/benchmark\u001b[0m\n",
            "[ 64%] Built target benchmark\n",
            "[ 65%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/convert-llama2c-to-ggml\u001b[0m\n",
            "[ 65%] Built target convert-llama2c-to-ggml\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/embedding\u001b[0m\n",
            "[ 66%] Built target embedding\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/finetune\u001b[0m\n",
            "[ 67%] Built target finetune\n",
            "[ 67%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gritlm\u001b[0m\n",
            "[ 68%] Built target gritlm\n",
            "[ 69%] \u001b[32mBuilding CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gguf-split\u001b[0m\n",
            "[ 70%] Built target gguf-split\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/infill\u001b[0m\n",
            "[ 71%] Built target infill\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 72%] Built target llama-bench\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
            "[ 74%] Built target llava\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
            "[ 74%] Built target llava_static\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llava-cli\u001b[0m\n",
            "[ 75%] Built target llava-cli\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/main.dir/main.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/main\u001b[0m\n",
            "[ 76%] Built target main\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/tokenize\u001b[0m\n",
            "[ 77%] Built target tokenize\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/parallel\u001b[0m\n",
            "[ 78%] Built target parallel\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/perplexity\u001b[0m\n",
            "[ 79%] Built target perplexity\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize\u001b[0m\n",
            "[ 81%] Built target quantize\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize-stats\u001b[0m\n",
            "[ 82%] Built target quantize-stats\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/save-load-state\u001b[0m\n",
            "[ 83%] Built target save-load-state\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/simple\u001b[0m\n",
            "[ 85%] Built target simple\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/passkey\u001b[0m\n",
            "[ 87%] Built target passkey\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/speculative\u001b[0m\n",
            "[ 88%] Built target speculative\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lookahead\u001b[0m\n",
            "[ 90%] Built target lookahead\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lookup\u001b[0m\n",
            "[ 91%] Built target lookup\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/gguf\u001b[0m\n",
            "[ 92%] Built target gguf\n",
            "[ 93%] \u001b[32mBuilding CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/train-text-from-scratch\u001b[0m\n",
            "[ 93%] Built target train-text-from-scratch\n",
            "[ 94%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/imatrix\u001b[0m\n",
            "[ 94%] Built target imatrix\n",
            "[ 95%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/server.dir/server.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/server\u001b[0m\n",
            "[ 95%] Built target server\n",
            "[ 96%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/export-lora\u001b[0m\n",
            "[ 97%] Built target export-lora\n",
            "[ 98%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/vdot\u001b[0m\n",
            "[ 99%] Built target vdot\n",
            "[100%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/q8dot\u001b[0m\n",
            "[100%] Built target q8dot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"huggingface_hub[cli]\"\n",
        "!huggingface-cli download mistralai/Mistral-7B-v0.1 --local-dir ./models --exclude \"*.bin\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh_YbTrw68nu",
        "outputId": "527aa99b-a326-4794-d599-bc2c7287f7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.10/dist-packages (0.21.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (24.0)\n",
            "Requirement already satisfied: InquirerPy==0.3.4 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[cli]) (0.3.4)\n",
            "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.43)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[cli]) (2024.2.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "Fetching 12 files: 100% 12/12 [00:00<00:00, 15.81it/s]\n",
            "/content/llama.cpp/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZRlIdur68qM",
        "outputId": "e1ce4628-d0f0-4ef7-a307-b428331620ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build\t\t\t       ggml-alloc.h\t    ggml-opencl.cpp\t\t Makefile\n",
            "build.zig\t\t       ggml-backend.c\t    ggml-opencl.h\t\t media\n",
            "ci\t\t\t       ggml-backend.h\t    ggml-quants.c\t\t models\n",
            "cmake\t\t\t       ggml-backend-impl.h  ggml-quants.h\t\t mypy.ini\n",
            "CMakeLists.txt\t\t       ggml.c\t\t    ggml-sycl.cpp\t\t Package.swift\n",
            "codecov.yml\t\t       ggml-common.h\t    ggml-sycl.h\t\t\t pocs\n",
            "common\t\t\t       ggml-cuda.cu\t    ggml_vk_generate_shaders.py  prompts\n",
            "convert-hf-to-gguf.py\t       ggml-cuda.h\t    ggml-vulkan.cpp\t\t README.md\n",
            "convert-llama-ggml-to-gguf.py  ggml.h\t\t    ggml-vulkan.h\t\t README-sycl.md\n",
            "convert-lora-to-ggml.py        ggml-impl.h\t    ggml-vulkan-shaders.hpp\t requirements\n",
            "convert-persimmon-to-gguf.py   ggml-kompute.cpp     gguf-py\t\t\t requirements.txt\n",
            "convert.py\t\t       ggml-kompute.h\t    grammars\t\t\t scripts\n",
            "docs\t\t\t       ggml-metal.h\t    kompute\t\t\t spm-headers\n",
            "examples\t\t       ggml-metal.m\t    kompute-shaders\t\t tests\n",
            "flake.lock\t\t       ggml-metal.metal     LICENSE\t\t\t unicode.cpp\n",
            "flake.nix\t\t       ggml-mpi.c\t    llama.cpp\t\t\t unicode.h\n",
            "ggml-alloc.c\t\t       ggml-mpi.h\t    llama.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CIoRxxN68sz",
        "outputId": "6894ca99-e8df-44eb-d9dd-285ab6418cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy~=1.24.4 (from -r ./requirements/requirements-convert.txt (line 1))\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece~=0.1.98 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert.txt (line 2)) (0.1.99)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements/requirements-convert.txt (line 3)) (4.38.2)\n",
            "Collecting gguf>=0.1.0 (from -r ./requirements/requirements-convert.txt (line 4))\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r ./requirements/requirements-convert.txt (line 5))\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch~=2.1.1 (from -r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m963.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops~=0.7.0 (from -r ./requirements/requirements-convert-hf-to-gguf.txt (line 3))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.21.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.1.0 (from torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r ./requirements/requirements-convert.txt (line 3)) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.1.1->-r ./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n",
            "Installing collected packages: triton, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gguf, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.3 which is incompatible.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed einops-0.7.0 gguf-0.6.0 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 protobuf-4.25.3 torch-2.1.2 triton-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 convert.py models/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtEdegOQ68vV",
        "outputId": "f367e7c1-ef64-4a7d-b658-8acc37e3d021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model file models/model-00001-of-00002.safetensors\n",
            "Loading model file models/model-00001-of-00002.safetensors\n",
            "Loading model file models/model-00002-of-00002.safetensors\n",
            "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models'))\n",
            "Found vocab files: {'spm': PosixPath('models/tokenizer.model'), 'bpe': None, 'hfft': PosixPath('models/tokenizer.json')}\n",
            "Loading vocab file PosixPath('models/tokenizer.model'), type 'spm'\n",
            "Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>\n",
            "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
            "Permuting layer 0\n",
            "Permuting layer 1\n",
            "Permuting layer 2\n",
            "Permuting layer 3\n",
            "Permuting layer 4\n",
            "Permuting layer 5\n",
            "Permuting layer 6\n",
            "Permuting layer 7\n",
            "Permuting layer 8\n",
            "Permuting layer 9\n",
            "Permuting layer 10\n",
            "Permuting layer 11\n",
            "Permuting layer 12\n",
            "Permuting layer 13\n",
            "Permuting layer 14\n",
            "Permuting layer 15\n",
            "Permuting layer 16\n",
            "Permuting layer 17\n",
            "Permuting layer 18\n",
            "Permuting layer 19\n",
            "Permuting layer 20\n",
            "Permuting layer 21\n",
            "Permuting layer 22\n",
            "Permuting layer 23\n",
            "Permuting layer 24\n",
            "Permuting layer 25\n",
            "Permuting layer 26\n",
            "Permuting layer 27\n",
            "Permuting layer 28\n",
            "Permuting layer 29\n",
            "Permuting layer 30\n",
            "Permuting layer 31\n",
            "model.embed_tokens.weight                        -> token_embd.weight                        | BF16   | [32000, 4096]\n",
            "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "lm_head.weight                                   -> output.weight                            | BF16   | [32000, 4096]\n",
            "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.norm.weight                                -> output_norm.weight                       | BF16   | [4096]\n",
            "Writing models/ggml-model-f32.gguf, format 0\n",
            "Ignoring added_tokens.json since model matches vocab size without it.\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "gguf: Setting special token type bos to 1\n",
            "gguf: Setting special token type eos to 2\n",
            "gguf: Setting special token type unk to 0\n",
            "gguf: Setting add_bos_token to True\n",
            "gguf: Setting add_eos_token to False\n",
            "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F32  | T+   3\n",
            "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   4\n",
            "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+   4\n",
            "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+   5\n",
            "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  10\n",
            "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+  11\n",
            "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  11\n",
            "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F32  | T+  11\n",
            "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  11\n",
            "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  11\n",
            "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  12\n",
            "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  12\n",
            "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+  12\n",
            "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  13\n",
            "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  14\n",
            "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  14\n",
            "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F32  | T+  14\n",
            "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  15\n",
            "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  15\n",
            "[ 20/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  15\n",
            "[ 21/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  15\n",
            "[ 22/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  16\n",
            "[ 23/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  17\n",
            "[ 24/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  18\n",
            "[ 25/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  18\n",
            "[ 26/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F32  | T+  18\n",
            "[ 27/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  19\n",
            "[ 28/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  19\n",
            "[ 29/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  20\n",
            "[ 30/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  20\n",
            "[ 31/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  25\n",
            "[ 32/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  25\n",
            "[ 33/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  26\n",
            "[ 34/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  26\n",
            "[ 35/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F32  | T+  26\n",
            "[ 36/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  27\n",
            "[ 37/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  27\n",
            "[ 38/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  27\n",
            "[ 39/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  27\n",
            "[ 40/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  28\n",
            "[ 41/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  29\n",
            "[ 42/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  30\n",
            "[ 43/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  30\n",
            "[ 44/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F32  | T+  30\n",
            "[ 45/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  34\n",
            "[ 46/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  35\n",
            "[ 47/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  35\n",
            "[ 48/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  35\n",
            "[ 49/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  40\n",
            "[ 50/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  42\n",
            "[ 51/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  45\n",
            "[ 52/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  45\n",
            "[ 53/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F32  | T+  45\n",
            "[ 54/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  45\n",
            "[ 55/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  45\n",
            "[ 56/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  45\n",
            "[ 57/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  45\n",
            "[ 58/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  46\n",
            "[ 59/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  48\n",
            "[ 60/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  49\n",
            "[ 61/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  49\n",
            "[ 62/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F32  | T+  49\n",
            "[ 63/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  50\n",
            "[ 64/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  50\n",
            "[ 65/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  50\n",
            "[ 66/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  50\n",
            "[ 67/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  55\n",
            "[ 68/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  60\n",
            "[ 69/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  60\n",
            "[ 70/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  60\n",
            "[ 71/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F32  | T+  60\n",
            "[ 72/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  61\n",
            "[ 73/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  61\n",
            "[ 74/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  61\n",
            "[ 75/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  61\n",
            "[ 76/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  62\n",
            "[ 77/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  63\n",
            "[ 78/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  64\n",
            "[ 79/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  64\n",
            "[ 80/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F32  | T+  64\n",
            "[ 81/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  64\n",
            "[ 82/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  65\n",
            "[ 83/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  65\n",
            "[ 84/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  65\n",
            "[ 85/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  70\n",
            "[ 86/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  72\n",
            "[ 87/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  75\n",
            "[ 88/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  75\n",
            "[ 89/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F32  | T+  75\n",
            "[ 90/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  75\n",
            "[ 91/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  77\n",
            "[ 92/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  77\n",
            "[ 93/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  77\n",
            "[ 94/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  79\n",
            "[ 95/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  85\n",
            "[ 96/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  85\n",
            "[ 97/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  85\n",
            "[ 98/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F32  | T+  85\n",
            "[ 99/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  86\n",
            "[100/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  86\n",
            "[101/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  86\n",
            "[102/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  86\n",
            "[103/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  87\n",
            "[104/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  88\n",
            "[105/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  89\n",
            "[106/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  89\n",
            "[107/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F32  | T+  89\n",
            "[108/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  89\n",
            "[109/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  89\n",
            "[110/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  90\n",
            "[111/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  90\n",
            "[112/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+  94\n",
            "[113/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  95\n",
            "[114/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  96\n",
            "[115/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  96\n",
            "[116/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F32  | T+  96\n",
            "[117/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  96\n",
            "[118/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  97\n",
            "[119/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  97\n",
            "[120/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  97\n",
            "[121/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  98\n",
            "[122/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  99\n",
            "[123/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+ 100\n",
            "[124/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 100\n",
            "[125/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 100\n",
            "[126/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 100\n",
            "[127/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 101\n",
            "[128/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+ 101\n",
            "[129/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 101\n",
            "[130/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 104\n",
            "[131/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 107\n",
            "[132/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+ 109\n",
            "[133/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 109\n",
            "[134/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 109\n",
            "[135/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 110\n",
            "[136/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 110\n",
            "[137/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 110\n",
            "[138/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 110\n",
            "[139/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 110\n",
            "[140/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 111\n",
            "[141/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+ 111\n",
            "[142/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+ 112\n",
            "[143/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 114\n",
            "[144/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 115\n",
            "[145/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+ 120\n",
            "[146/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 120\n",
            "[147/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 124\n",
            "[148/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 125\n",
            "[149/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 125\n",
            "[150/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+ 125\n",
            "[151/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+ 125\n",
            "[152/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 130\n",
            "[153/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 131\n",
            "[154/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+ 132\n",
            "[155/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 132\n",
            "[156/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 132\n",
            "[157/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 132\n",
            "[158/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 132\n",
            "[159/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+ 132\n",
            "[160/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+ 132\n",
            "[161/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 133\n",
            "[162/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 134\n",
            "[163/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+ 135\n",
            "[164/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 135\n",
            "[165/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 135\n",
            "[166/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 135\n",
            "[167/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 136\n",
            "[168/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+ 136\n",
            "[169/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+ 136\n",
            "[170/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 137\n",
            "[171/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 138\n",
            "[172/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+ 139\n",
            "[173/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 139\n",
            "[174/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 139\n",
            "[175/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 139\n",
            "[176/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 139\n",
            "[177/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+ 139\n",
            "[178/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+ 141\n",
            "[179/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 141\n",
            "[180/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 142\n",
            "[181/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+ 143\n",
            "[182/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 143\n",
            "[183/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 143\n",
            "[184/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 143\n",
            "[185/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 144\n",
            "[186/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+ 144\n",
            "[187/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+ 144\n",
            "[188/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 145\n",
            "[189/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 146\n",
            "[190/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+ 147\n",
            "[191/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 147\n",
            "[192/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 147\n",
            "[193/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 147\n",
            "[194/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 151\n",
            "[195/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+ 151\n",
            "[196/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+ 151\n",
            "[197/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+ 152\n",
            "[198/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+ 153\n",
            "[199/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+ 154\n",
            "[200/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type F32  | T+ 154\n",
            "[201/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F32  | T+ 154\n",
            "[202/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F32  | T+ 154\n",
            "[203/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type F32  | T+ 155\n",
            "[204/291] Writing tensor output.weight                          | size  32000 x   4096  | type F32  | T+ 157\n",
            "[205/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+ 158\n",
            "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 158\n",
            "[207/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 159\n",
            "[208/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 160\n",
            "[209/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+ 161\n",
            "[210/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+ 161\n",
            "[211/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 161\n",
            "[212/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 166\n",
            "[213/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 167\n",
            "[214/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 172\n",
            "[215/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 172\n",
            "[216/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 172\n",
            "[217/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 172\n",
            "[218/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 172\n",
            "[219/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 172\n",
            "[220/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 172\n",
            "[221/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 173\n",
            "[222/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 175\n",
            "[223/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 176\n",
            "[224/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 176\n",
            "[225/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 176\n",
            "[226/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 177\n",
            "[227/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 177\n",
            "[228/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 177\n",
            "[229/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 177\n",
            "[230/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 178\n",
            "[231/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 179\n",
            "[232/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 180\n",
            "[233/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 180\n",
            "[234/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 180\n",
            "[235/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 180\n",
            "[236/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 180\n",
            "[237/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 180\n",
            "[238/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 180\n",
            "[239/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 181\n",
            "[240/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 182\n",
            "[241/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 183\n",
            "[242/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 183\n",
            "[243/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 183\n",
            "[244/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 183\n",
            "[245/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 184\n",
            "[246/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 184\n",
            "[247/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 184\n",
            "[248/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 185\n",
            "[249/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 186\n",
            "[250/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 187\n",
            "[251/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 187\n",
            "[252/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 187\n",
            "[253/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 187\n",
            "[254/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 188\n",
            "[255/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 188\n",
            "[256/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 188\n",
            "[257/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 192\n",
            "[258/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 192\n",
            "[259/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 193\n",
            "[260/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 193\n",
            "[261/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 193\n",
            "[262/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 194\n",
            "[263/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 194\n",
            "[264/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 194\n",
            "[265/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 194\n",
            "[266/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 196\n",
            "[267/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 196\n",
            "[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 197\n",
            "[269/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 197\n",
            "[270/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 197\n",
            "[271/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 197\n",
            "[272/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 201\n",
            "[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 201\n",
            "[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 201\n",
            "[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 202\n",
            "[276/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 203\n",
            "[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 204\n",
            "[278/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 204\n",
            "[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 204\n",
            "[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 204\n",
            "[281/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 204\n",
            "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 205\n",
            "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+ 205\n",
            "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+ 206\n",
            "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+ 206\n",
            "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 207\n",
            "[287/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type F32  | T+ 207\n",
            "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F32  | T+ 207\n",
            "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F32  | T+ 208\n",
            "[290/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type F32  | T+ 208\n",
            "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 208\n",
            "Wrote models/ggml-model-f32.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ./build/bin/quantize ./models/ggml-model-f32.gguf ./models/ggml-model-Q4_K_M.gguf Q4_K_M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpwqrpRh68yB",
        "outputId": "36a3667f-ae13-4752-bd47-7e83d350953e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 2491 (fa046eaf)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing './models/ggml-model-f32.gguf' to './models/ggml-model-Q4_K_M.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./models/ggml-model-f32.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 0\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - type  f32:  291 tensors\n",
            "llama_model_quantize_internal: meta size = 734944 bytes\n",
            "[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f32, converting to q4_K .. size =   500.00 MiB ->    70.31 MiB\n",
            "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[  20/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[  22/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  23/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  24/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  25/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  26/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  27/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  28/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[  29/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[  31/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  32/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  33/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  34/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  35/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  36/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  37/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[  38/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  40/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  41/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  42/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  43/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  44/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  45/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  46/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  47/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  49/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  50/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  51/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  52/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  53/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  54/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  55/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  56/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[  58/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  59/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  60/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  61/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  62/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  63/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  64/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[  65/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  67/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  68/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  69/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  70/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  71/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  72/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  73/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  74/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  76/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  77/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  78/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  79/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  80/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  81/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  82/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  83/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[  85/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  86/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  87/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  88/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  89/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  90/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  91/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[  92/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  94/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  95/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[  96/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  97/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[  98/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[  99/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 100/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 101/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 103/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 104/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 105/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 106/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 107/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 108/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 109/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 110/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 112/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 113/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 114/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 115/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 116/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 117/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 118/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 119/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 121/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 122/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 123/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 124/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 125/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 126/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 127/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 128/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 130/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 131/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 132/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 133/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 134/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 135/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 136/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 137/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 138/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 139/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 140/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 141/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 142/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 143/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 144/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 145/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 146/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 147/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 148/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 149/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 150/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 151/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 152/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 153/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 154/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 155/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 156/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 157/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 158/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 159/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 160/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 161/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 162/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 163/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 164/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 165/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 166/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 167/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 168/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 169/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 170/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 171/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 172/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 173/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 174/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 175/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 176/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 177/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 178/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 179/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 180/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 181/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 182/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 183/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 184/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 185/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 186/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 187/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 188/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 189/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 190/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 191/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 192/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 193/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 194/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 195/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 196/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 197/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 198/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 199/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 200/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 201/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 202/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 203/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 204/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f32, converting to q6_K .. size =   500.00 MiB ->   102.54 MiB\n",
            "[ 205/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 206/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 207/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 208/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 211/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 212/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 213/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 214/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 215/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 216/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 217/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 218/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 219/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 220/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 221/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 222/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 223/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 224/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 225/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 226/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 227/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 228/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 229/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 230/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 231/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 232/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 233/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 234/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 235/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 236/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 237/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 238/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 239/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 240/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 241/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 242/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 243/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 244/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 245/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 246/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 247/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 248/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 249/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 250/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 251/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 252/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 253/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 254/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 255/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 256/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 257/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 258/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 259/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 260/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 261/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 262/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 263/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 264/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 265/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 266/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 267/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 268/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 269/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 270/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 271/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 272/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 273/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 274/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 275/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 276/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 277/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 278/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 279/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 280/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 281/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 282/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 283/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f32, converting to q6_K .. size =   224.00 MiB ->    45.94 MiB\n",
            "[ 284/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 285/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f32, converting to q4_K .. size =   224.00 MiB ->    31.50 MiB\n",
            "[ 286/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 287/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q4_K .. size =    16.00 MiB ->     2.25 MiB\n",
            "[ 288/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 289/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f32, converting to q4_K .. size =    64.00 MiB ->     9.00 MiB\n",
            "[ 290/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f32, converting to q6_K .. size =    16.00 MiB ->     3.28 MiB\n",
            "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "llama_model_quantize_internal: model size  = 27625.02 MB\n",
            "llama_model_quantize_internal: quant size  =  4165.37 MB\n",
            "\n",
            "main: quantize time = 591608.23 ms\n",
            "main:    total time = 591608.23 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mzINHA3W6803"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}